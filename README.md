# üìö Sistema RAG para Control de Calidad del Conocimiento Interno

<div align="center">
  <img src="https://img.shields.io/badge/Python-3.8%2B-blue" alt="Python 3.8+">
  <img src="https://img.shields.io/badge/Framework-LangChain-yellow" alt="LangChain">
  <img src="https://img.shields.io/badge/Vector%20Store-FAISS-ff69b4" alt="FAISS">
  <img src="https://img.shields.io/badge/LLM-llama--3.1--nemotron--nano--8b--v1-orange" alt="LLM Model">
</div>

üë®‚Äçüíª **Autor**: Ronald Castillo Capino  
üìß **Contacto**: [ron.h.castillo@gmail.com](mailto:ron.h.castillo@gmail.com)

> üí° Este proyecto implementa un sistema avanzado de Preguntas y Respuestas (Q&A) que combina recuperaci√≥n de informaci√≥n con generaci√≥n de lenguaje natural, garantizando respuestas precisas, verificables y basadas en documentos espec√≠ficos.

## üöÄ Descripci√≥n del Proyecto

Este sistema **RAG (Retrieval-Augmented Generation)** est√° dise√±ado para proporcionar respuestas precisas y contextualizadas mediante la combinaci√≥n de:

- üîç **Recuperaci√≥n de informaci√≥n** avanzada de documentos t√©cnicos
- üß† **Generaci√≥n de respuestas** utilizando el modelo local `Llama-3.1-Nemotron-Nano-4B-v1.1` con aceleraci√≥n por GPU
- üìä **Evaluaci√≥n autom√°tica** de la calidad con m√©tricas RAGAS
- üöÄ **Aceleraci√≥n por GPU** para un rendimiento √≥ptimo en inferencia local

üí° **Caso de Uso Principal**: Sistema experto de preguntas y respuestas sobre el libro "An Introduction to Statistical Learning with Applications in Python", permitiendo a los usuarios obtener explicaciones claras y precisas sobre conceptos de aprendizaje estad√≠stico.

### üöÄ Desempe√±o con Aceleraci√≥n por GPU

El sistema est√° optimizado para aprovechar al m√°ximo la capacidad de procesamiento en paralelo de tu GPU local, ofreciendo tiempos de respuesta r√°pidos con el modelo Llama-3.1-Nemotron-Nano-8B. La configuraci√≥n incluye:

- **Contexto extendido**: 16,384 tokens
- **Capas GPU**: 30 capas (ajustable seg√∫n VRAM)
- **Procesamiento por lotes**: 512 tokens
- **Hilos de CPU**: 8 hilos

### üîß Requisitos de Hardware

| Componente | M√≠nimo | Recomendado |
|------------|--------|-------------|
| **GPU** | NVIDIA RTX 3060 | NVIDIA RTX 4090 o superior |
| **VRAM** | 12GB | 24GB+ |
| **RAM del Sistema** | 32GB | 64GB+ |
| **Almacenamiento** | 15GB libres | SSD NVMe |

### ‚öôÔ∏è Configuraci√≥n √ìptima

1. **Controladores NVIDIA**
   ```bash
   # Verificar instalaci√≥n de controladores
   nvidia-smi
   # Versi√≥n m√≠nima recomendada: 525.60.13
   ```

2. **Bibliotecas CUDA**
   - CUDA Toolkit 11.7 o superior
   - cuDNN 8.5 o superior
   - Verificar instalaci√≥n:
     ```bash
     nvcc --version
     ```

### üöÄ Optimizaciones Implementadas

- **Inferencia Acelerada por GPU**
  - Todas las operaciones del modelo se ejecutan en la GPU
  - Soporte para CUDA y cuBLAS para operaciones matriciales

- **Gesti√≥n Eficiente de Memoria**
  - Carga selectiva de capas del modelo
  - Optimizaci√≥n de memoria intermedia
  - Soporte para precisi√≥n mixta (FP16/FP32)

- **Procesamiento por Lotes**
  - Procesamiento paralelo de m√∫ltiples consultas
  - Ajuste autom√°tico del tama√±o de lote seg√∫n la VRAM disponible

### üìä Rendimiento Esperado

| Configuraci√≥n | Tokens/seg | Memoria GPU |
|---------------|------------|-------------|
| RTX 3060 (12GB) | 18-25 | ~12GB |
| RTX 3090 (24GB) | 30-40 | ~22GB |
| RTX 4090 (24GB) | 45-60 | ~24GB |

*Nota: El rendimiento puede variar seg√∫n la carga del sistema y la configuraci√≥n espec√≠fica.*

### Caracter√≠sticas Clave

- ‚úÖ **Respuestas Basadas en Contexto**: Cada respuesta est√° respaldada por fragmentos espec√≠ficos del documento
- üîç **B√∫squeda Sem√°ntica**: Encuentra informaci√≥n relevante incluso con consultas en lenguaje natural
- üìà **Evaluaci√≥n Continua**: Sistema integrado para medir y mejorar la calidad de las respuestas
- üöÄ **Rendimiento Optimizado**: Dise√±ado para funcionar eficientemente en hardware est√°ndar

## üéØ Objetivo

Desarrollar un asistente de IA que:

‚úÖ Proporcione respuestas precisas basadas en documentos espec√≠ficos  
‚úÖ Mantenga la trazabilidad de las fuentes de informaci√≥n  
‚úÖ Eval√∫e autom√°ticamente la calidad de las respuestas  
‚úÖ Sea f√°cil de implementar y mantener

## üèóÔ∏è Arquitectura del Sistema

```mermaid
graph TD
    A[Usuario] -->|Pregunta| B[Procesamiento de Texto]
    B --> C[Generaci√≥n de Embeddings]
    C --> D[Base de Vectores FAISS]
    D -->|Contexto Relevante| E[Modelo de Lenguaje]
    E -->|Respuesta| A
    F[Documentos] -->|Procesamiento| G[Base de Conocimiento]
    G --> D
    H[M√≥dulo de Evaluaci√≥n] <-->|M√©tricas| E
```

### üîß Componentes Principales

#### 1. **Procesamiento de Documentos**
- **Extracci√≥n de Texto**: Utiliza PyPDF2 para extraer texto de documentos PDF
- **Limpieza de Texto**: Eliminaci√≥n de caracteres especiales, normalizaci√≥n de espacios
- **Tokenizaci√≥n**: Divisi√≥n del texto en unidades significativas
- **Fragmentaci√≥n Sem√°ntica**:
  - Tama√±o de fragmento: 4000 caracteres
  - Solapamiento: 200 caracteres
  - Preservaci√≥n de contexto entre fragmentos

#### 2. **Modelo de Embeddings**
- **Modelo**: `sentence-transformers/all-MiniLM-L6-v2`
- **Dimensi√≥n de Embeddings**: 384
- **Normalizaci√≥n**: Vectores unitarios para similitud coseno
- **Rendimiento**: Optimizado para equilibrio entre precisi√≥n y velocidad

#### 3. **Almacenamiento Vectorial (FAISS)**
- **√çndice**: `IndexFlatL2` para b√∫squeda exacta
- **M√©tricas**: Distancia euclidiana (L2)
- **Optimizaciones**:
  - B√∫squeda por lotes
  - Filtrado por umbral de similitud
  - Recuperaci√≥n de los 5 mejores resultados

#### 4. **Modelo de Lenguaje Local**
- **Modelo Base**: `Llama-3.1-Nemotron-Nano-8B-v1`
- **Ejecuci√≥n**:
  - Ejecutado localmente a trav√©s de LM Studio
  - Aceleraci√≥n por GPU para m√°ximo rendimiento
  - Versi√≥n cuantizada (Q4_K_S) para eficiencia
- **Configuraci√≥n**:
  - Temperatura: 0.1 (para respuestas deterministas)
  - Tokens m√°ximos: 2048
  - Contexto: 16,384 tokens
  - Capas GPU: 30 (ajustable seg√∫n VRAM)
  - Procesamiento por lotes: 512 tokens
  - Hilos CPU: 8
- **Prompt Engineering**:
  - Instrucciones claras para el modelo
  - Formato estructurado de respuestas
  - Manejo de incertidumbre

#### 5. **M√≥dulo de Evaluaci√≥n**
- **M√©tricas RAGAS**:
  - Faithfulness (Fidelidad)
  - Answer Relevancy (Relevancia de la respuesta)
  - Context Precision (Precisi√≥n del contexto)
  - Context Recall (Recuperaci√≥n del contexto)
  - Answer Similarity (Similitud de respuestas)
  - Answer Correctness (Correcci√≥n de respuestas)
  - Harmfulness (Contenido da√±ino)
- **Reportes**:
  - Salida detallada en consola
  - Archivo de registro estructurado
  - Estad√≠sticas agregadas

#### 6. **API y Servicios**
- **Interfaz de L√≠nea de Comandos (CLI)**
- **API RESTful** (opcional)
- **Integraci√≥n con LM Studio**
  - Endpoint: `http://localhost:1234/v1`
  - Soporte para streaming
  - Manejo de tiempo de espera

### üîÑ Flujo de Datos

1. **Ingreso de Consulta**
   - El usuario ingresa una pregunta en lenguaje natural
   - La consulta se normaliza y procesa

2. **B√∫squeda Sem√°ntica**
   - La consulta se convierte en embedding
   - Se buscan los fragmentos m√°s similares en FAISS
   - Se recuperan los 5 fragmentos m√°s relevantes

3. **Generaci√≥n de Respuesta**
   - Los fragmentos recuperados se combinan con el prompt
   - El modelo de lenguaje genera una respuesta contextualizada
   - Se aplican filtros de seguridad y calidad

4. **Retroalimentaci√≥n y Mejora**
   - La interacci√≥n se registra para evaluaci√≥n
   - Las m√©tricas se calculan y almacenan
   - El sistema se ajusta seg√∫n el rendimiento

## ‚öôÔ∏è Configuraci√≥n del Entorno

El archivo `.env` contiene las siguientes configuraciones clave:

```ini
# Configuraci√≥n del Modelo
MODEL_NAME=llama-3.1-nemotron-nano-8b-v1
LOCAL_MODEL_PATH=ruta/al/modelo.gguf

# Configuraci√≥n de Rendimiento
N_CTX=16384
N_GPU_LAYERS=30
N_BATCH=512
N_THREADS=8

# Configuraci√≥n de Generaci√≥n
LLM_TEMPERATURE=0.1
MAX_TOKENS=2048

# Configuraci√≥n de Fragmentaci√≥n
CHUNK_SIZE=4000
CHUNK_OVERLAP=200
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Configuraci√≥n de Recuperaci√≥n
TOP_K_RETRIEVAL=5
CONTEXT_WINDOW=131072
```

## üìÅ Estructura del Proyecto

```
.
‚îú‚îÄ‚îÄ data/                           # Directorio de datos
‚îÇ   ‚îî‚îÄ‚îÄ PDF-GenAI-Challenge.pdf    # Documento fuente para el sistema RAG
‚îÇ
‚îú‚îÄ‚îÄ evaluate/                      # Resultados de evaluaci√≥n
‚îÇ   ‚îî‚îÄ‚îÄ evaluation_report.txt      # Reporte detallado de m√©tricas
‚îÇ
‚îú‚îÄ‚îÄ .env                   # Plantilla de configuraci√≥n
‚îú‚îÄ‚îÄ main.py                       # Punto de entrada principal
‚îú‚îÄ‚îÄ evaluate.py                   # M√≥dulo de evaluaci√≥n
‚îú‚îÄ‚îÄ utils.py                      # Utilidades y helpers
‚îú‚îÄ‚îÄ requirements.txt              # Dependencias del proyecto
‚îî‚îÄ‚îÄ README.md                     # Documentaci√≥n
```

### üìã Archivos Principales

#### `main.py`
Punto de entrada principal que implementa:
- Carga y procesamiento de documentos
- Interfaz de l√≠nea de comandos
- Integraci√≥n de componentes RAG
- Manejo de errores y logging

#### `evaluate.py`
M√≥dulo de evaluaci√≥n que incluye:
- Implementaci√≥n de m√©tricas RAGAS
- Generaci√≥n de reportes detallados
- Herramientas para an√°lisis comparativo
- Visualizaci√≥n de resultados

#### `utils.py`
Funciones auxiliares para:
- Procesamiento de texto y PDF
- Manejo de embeddings
- Utilidades de sistema
- Configuraci√≥n y logging

#### `requirements.txt`
Lista de dependencias con versiones espec√≠ficas para garantizar compatibilidad.

#### `.env`
Archivo de configuraci√≥n que debe contener:
- Rutas a modelos
- Configuraciones de ejecuci√≥n
- Par√°metros del sistema

## ‚öôÔ∏è Instalaci√≥n y Configuraci√≥n

### 1. üõ†Ô∏è Requisitos Previos

- Python 3.8 o superior
- pip (gestor de paquetes de Python)
- Git (opcional, para clonar el repositorio)
- LM Studio (para ejecutar modelos locales)
- Al menos 8GB de RAM (16GB recomendado)

### 2. üèóÔ∏è Configuraci√≥n del Entorno

```bash
# 1. Clonar el repositorio (opcional)
git clone <repo-url>
cd rag_challenge

# 2. Crear y activar entorno virtual
python -m venv rag_env
source rag_env/bin/activate  # Linux/Mac
# o
rag_env\Scripts\activate     # Windows

# 3. Actualizar pip
python -m pip install --upgrade pip

# 4. Instalar dependencias
pip install -r requirements.txt
```

### 3. üîê Configuraci√≥n de Variables de Entorno

1. Copiar el archivo de ejemplo:
   ```bash
   cp .env.example .env
   ```

2. Editar el archivo `.env` con tus configuraciones:
   ```env
   # ===== Configuraci√≥n del Modelo =====
   LOCAL_LLM_URL=http://localhost:1234/v1
   MODEL_NAME=llama-3.1-nemotron-nano-8b-v1
   
   # ===== Configuraci√≥n de Embeddings =====
   EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
   
   # ===== Configuraci√≥n de Procesamiento =====
   CHUNK_SIZE=4000
   CHUNK_OVERLAP=200
   TOP_K_RETRIEVAL=5
   
   # ===== Configuraci√≥n del Modelo =====
   LLM_TEMPERATURE=0.1
   MAX_TOKENS=1024
   CONTEXT_WINDOW=2048
   
   # ===== Configuraci√≥n de Evaluaci√≥n =====
   EVAL_SAMPLES=5
   EVALUATION_OUTPUT_PATH=evaluate/evaluation_report.txt
   
   # ===== Configuraci√≥n del Sistema =====
   LOG_LEVEL=INFO
   CACHE_DIR=./cache
   ```

### 4. üì¶ Dependencias Principales

El archivo `requirements.txt` contiene todas las dependencias necesarias:

```
# Procesamiento de documentos
PyPDF2>=3.0.0
python-dotenv>=1.0.0

# Procesamiento de lenguaje natural
sentence-transformers>=2.2.2
numpy>=1.24.0

# Almacenamiento vectorial
faiss-cpu>=1.7.4  # o faiss-gpu si tienes CUDA

# Modelo de lenguaje
llama-cpp-python>=0.2.0

# Evaluaci√≥n
ragas>=0.0.21
datasets>=2.14.0

# Utilidades
tqdm>=4.65.0
colorama>=0.4.6
python-dotenv>=1.0.0
```

### 5. üöÄ Configuraci√≥n del Modelo Local

#### Descarga e Instalaci√≥n del Modelo

1. **Descargar el modelo**:
   - Nombre: `Llama-3.1-Nemotron-Nano-4B-v1.1-GGUF`
   - Tama√±o: ~2.5GB (versi√≥n cuantizada Q4_K_M)
   - Ubicaci√≥n por defecto: `C:\Users\[usuario]\.cache\lm-studio\models\`

2. **Configuraci√≥n en LM Studio**:
   - Abrir LM Studio y seleccionar "Download a model"
   - Buscar: `llama-3.1-nemotron-nano-8b-v1-GGUF`
   - Descargar la versi√≥n `Q4_K_M` para el mejor equilibrio entre rendimiento y calidad

3. **Habilitar Aceleraci√≥n por GPU**:
   - Ir a Configuraci√≥n ‚Üí Modelo
   - Seleccionar "Auto" o tu GPU espec√≠fica en "GPU Layers"
   - Activar "Use CUDA"
   - Establecer "Context Length" a 2048 tokens

4. **Iniciar el Servidor Local**:
   - Ir a la pesta√±a "Local Server"
   - Asegurarse de que "GPU Offload" est√© activado
   - Hacer clic en "Start Server"
   - Verificar que la URL sea `http://localhost:1234/v1`

5. **Verificar la Configuraci√≥n de GPU**:
   - Abrir el Administrador de Tareas de Windows
   - Ir a la pesta√±a "Rendimiento"
   - Verificar que la GPU muestre actividad durante la inferencia
   - Confirmar que la memoria de GPU se est√© utilizando

#### Configuraci√≥n Recomendada para √ìptimo Rendimiento

```env
# En tu archivo .env
LOCAL_LLM_URL=http://localhost:1234/v1
MODEL_NAME=llama-3.1-nemotron-nano-8b-v1
LLM_TEMPERATURE=0.1
MAX_TOKENS=1024
CONTEXT_WINDOW=2048
```

2. **Preparar los datos**:
   ```bash
   mkdir -p data
   cp /ruta/a/tu/documento.pdf data/PDF-GenAI-Challenge.pdf
   ```

3. **Verificar la instalaci√≥n**:
   ```bash
   python -c "import torch; print(f'PyTorch: {torch.__version__}')"
   python -c "import transformers; print(f'Transformers: {transformers.__version__}')"
   ```

### 6. üß™ Prueba R√°pida

```bash
# Ejecutar una consulta de prueba
echo "¬øQu√© es el aprendizaje autom√°tico?" | python main.py

# Ejecutar evaluaci√≥n b√°sica
python evaluate.py --samples 3
```

### üìù Notas de Instalaci√≥n

- Para mejor rendimiento, se recomienda usar una GPU compatible con CUDA
- El primer inicio puede tardar varios minutos mientras se descargan los modelos
- Verifica que el puerto 1234 est√© disponible para LM Studio

## üíª Uso del Sistema

### 1. Modos de Operaci√≥n

#### Modo Interactivo
```bash
# Iniciar el sistema en modo interactivo
python main.py

# Ejemplo de sesi√≥n:
Bienvenido al Sistema RAG de Control de Calidad
> ¬øSobre qu√© tema te gustar√≠a consultar?
```

#### Modo por L√≠nea de Comandos
```bash
# Hacer una pregunta espec√≠fica
python main.py --pregunta "¬øQu√© es el trade-off entre sesgo y varianza?"

# Procesar un archivo con m√∫ltiples preguntas
python main.py --archivo preguntas.txt --salida respuestas.json

# Opciones adicionales
python main.py \
  --pregunta "Explique la regresi√≥n lineal" \
  --temperatura 0.3 \
  --max-tokens 500 \
  --mostrar-fuentes
```

### 2. Ejemplos de Preguntas

#### Conceptos B√°sicos
```
- "¬øQu√© es el aprendizaje supervisado?"
- "Explique el concepto de validaci√≥n cruzada"
- "¬øCu√°les son las ventajas de los √°rboles de decisi√≥n?"
```

#### Comparaciones
```
- "Compare ridge y lasso regression"
- "Diferencia entre bagging y boosting"
- "Ventajas de SVM sobre regresi√≥n log√≠stica"
```

#### Aplicaciones Pr√°cticas
```
- "¬øC√≥mo manejar datos faltantes en un dataset?"
- "T√©cnicas para tratar el desbalance de clases"
- "M√©todos de selecci√≥n de caracter√≠sticas"
```

### 3. Opciones Avanzadas

#### Configuraci√≥n de B√∫squeda
```bash
# Ajustar el n√∫mero de fragmentos recuperados
python main.py --top-k 3

# Cambiar el umbral de similitud (0-1)
python main.py --umbral-similitud 0.75
```

#### Control de Salida
```bash
# Mostrar solo la respuesta sin fuentes
python main.py --pregunta "..." --formato simple

# Generar salida en formato JSON
python main.py --pregunta "..." --formato json

# Guardar resultados en un archivo
python main.py --pregunta "..." --salida resultado.txt
```

## üìä Evaluaci√≥n del Sistema

### 1. M√©tricas Implementadas

El sistema utiliza el framework RAGAS para evaluar la calidad de las respuestas con las siguientes m√©tricas:

| M√©trica | Rango √ìptimo | Descripci√≥n |
|---------|--------------|-------------|
| **Faithfulness** | 0.8 - 1.0 | Mide si la respuesta se basa √∫nicamente en el contexto |
| **Answer Relevancy** | > 0.7 | Eval√∫a la relevancia de la respuesta |
| **Context Precision** | > 0.6 | Precisi√≥n de los fragmentos recuperados |
| **Context Recall** | > 0.7 | Capacidad de recuperar informaci√≥n relevante |
| **Answer Similarity** | > 0.75 | Comparaci√≥n con respuestas de referencia |
| **Answer Correctness** | > 0.8 | Precisi√≥n f√°ctica de la respuesta |
| **Harmfulness** | < 0.2 | Detecci√≥n de contenido potencialmente da√±ino |

### 2. Ejecuci√≥n de la Evaluaci√≥n

#### Evaluaci√≥n B√°sica
```bash
# Evaluar con 5 ejemplos (valor por defecto)
python evaluate.py
```

#### Evaluaci√≥n Personalizada
```bash
# Especificar n√∫mero de muestras
python evaluate.py --muestras 10

# Evaluar m√©tricas espec√≠ficas
python evaluate.py --metricas fidelidad relevancia

# Generar reporte en formato JSON
python evaluate.py --formato json --salida resultados.json
```

#### Evaluaci√≥n con Conjunto de Datos Personalizado
```bash
# Usar un archivo JSON con preguntas y respuestas de referencia
python evaluate.py --dataset datos_evaluacion.json
```

### 3. Interpretaci√≥n de Resultados

#### Ejemplo de Salida
```
========================================
   RESULTADOS DE LA EVALUACI√ìN RAGAS   
========================================

- Faithfulness: 0.87
  ‚úì Excelente: La respuesta se basa completamente en el contexto

- Answer Relevancy: 0.82
  ‚úì Muy buena: La respuesta es altamente relevante a la pregunta

- Context Precision: 0.75
  ‚úì Buena: La mayor√≠a de los fragmentos recuperados son relevantes

- Context Recall: 0.68
  ‚úì Aceptable: Se recupera la mayor parte de la informaci√≥n relevante

- Answer Similarity: 0.79
  ‚úì Buena: La respuesta es similar a la referencia esperada

- Answer Correctness: 0.83
  ‚úì Muy buena: La informaci√≥n proporcionada es correcta

- Harmfulness: 0.05
  ‚úì Seguro: No se detect√≥ contenido da√±ino

----------------------------------------
Puntuaci√≥n Promedio: 0.79
Estado General: Buen rendimiento
----------------------------------------

üìù Recomendaciones:
- Mejorar la recuperaci√≥n de contexto para aumentar el recall
- Verificar posibles casos de informaci√≥n faltante en las respuestas
```

### 4. An√°lisis de Resultados

#### Archivos Generados
- `evaluate/evaluation_report.txt`: Reporte detallado en formato de texto
- `evaluate/metrics/`: Directorio con m√©tricas hist√≥ricas
- `evaluate/failed_cases.json`: Casos que requieren revisi√≥n manual

#### Visualizaci√≥n de M√©tricas
```bash
# Generar gr√°ficos de tendencias
python evaluate.py --graficar
```

### 5. Personalizaci√≥n de la Evaluaci√≥n

Puedes modificar el archivo `evaluate.py` para:
- Ajustar los umbrales de las m√©tricas
- Agregar nuevas m√©tricas personalizadas
- Cambiar el conjunto de datos de evaluaci√≥n
- Modificar los prompts de evaluaci√≥n

### 6. Integraci√≥n Continua

El sistema puede integrarse en pipelines CI/CD para monitorear el rendimiento a lo largo del tiempo:

```yaml
# Ejemplo de configuraci√≥n para GitHub Actions
name: Evaluaci√≥n RAG

on: [push, pull_request]

jobs:
  evaluate:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    - name: Run evaluation
      run: |
        python evaluate.py --samples 10
    - name: Upload results
      uses: actions/upload-artifact@v3
      with:
        name: evaluation-results
        path: evaluate/
```

## üö® Soluci√≥n de Problemas Comunes

### 1. Problemas de Inicio

**S√≠ntoma**: Error al iniciar la aplicaci√≥n
```
[ERROR] No se pudo cargar el modelo: ConnectionError
```
**Soluci√≥n**:
1. Verifica que el servidor de LM Studio est√© en ejecuci√≥n
2. Confirma la URL en el archivo `.env`
3. Revisa los logs para mensajes adicionales

### 2. Problemas de Rendimiento

**S√≠ntoma**: Respuestas lentas o tiempo de espera agotado
```
[WARNING] La generaci√≥n est√° tardando m√°s de lo esperado
```
**Soluci√≥n**:
```bash
# Reducir la carga del sistema
export TOP_K_RETRIEVAL=3
export MAX_TOKENS=512

# Para sistemas con GPU limitada
export CUDA_VISIBLE_DEVICES=0  # Usar solo la primera GPU
```

### 3. Optimizaci√≥n de GPU para M√°ximo Rendimiento

**S√≠ntoma**: Bajo uso de GPU o rendimiento por debajo de lo esperado
```
[INFO] Uso de GPU por debajo del 50%
```
**Soluci√≥n**:
1. Verifica que CUDA est√© correctamente instalado:
   ```bash
   nvidia-smi  # Debe mostrar el uso de GPU
   python -c "import torch; print(f'CUDA disponible: {torch.cuda.is_available()}')
   ```
2. Ajusta el tama√±o de lote para mejor uso de GPU:
   ```python
   # En tu configuraci√≥n
   os.environ['BATCH_SIZE'] = '8'  # Aumentar para mejor uso de GPU
   ```
3. Habilita la precisi√≥n mixta:
   ```python
   from torch.cuda.amp import autocast
   
   with autocast():
       # Tu c√≥digo de inferencia aqu√≠
   ```

### 4. Problemas de Memoria

**S√≠ntoma**: Errores de memoria insuficiente
```
[ERROR] Error al asignar memoria: CUDA out of memory
```
**Soluci√≥n**:
1. Reduce el tama√±o del lote:
   ```python
   # En tu c√≥digo Python
   import os
   os.environ['BATCH_SIZE'] = '4'
   ```
2. Usa precisi√≥n mixta:
   ```python
   model.half()  # Usar precisi√≥n FP16
   ```
3. Libera memoria de la GPU:
   ```python
   import torch
   torch.cuda.empty_cache()
   ```

### 4. Problemas con los Embeddings

**S√≠ntoma**: Baja precisi√≥n en las b√∫squedas
```
[WARNING] No se encontraron fragmentos relevantes
```
**Soluci√≥n**:
- Verifica que el modelo de embeddings est√© correctamente cargado
- Considera ajustar el umbral de similitud
- Revisa la calidad del texto de entrada

## üöÄ Pr√≥ximas Mejoras

### Mejoras en Curso

1. **Optimizaci√≥n de Rendimiento**
   - [ ] Soporte para cuantizaci√≥n de modelos
   - [ ] Cach√© de embeddings en disco
   - [ ] Procesamiento por lotes mejorado

2. **Nuevas Funcionalidades**
   - [ ] Soporte para m√∫ltiples formatos de documentos
   - [ ] Integraci√≥n con m√°s bases de datos vectoriales
   - [ ] Sistema de plugins para extensiones

3. **Mejoras en la Interfaz**
   - [ ] Interfaz web interactiva
   - [ ] Panel de control de m√©tricas
   - [ ] Visualizaci√≥n de grafos de conocimiento

### Caracter√≠sticas Futuras

1. **Soporte Multimodal**
   - Procesamiento de im√°genes y tablas
   - B√∫squeda sem√°ntica en m√∫ltiples formatos
   - Respuestas enriquecidas con visualizaciones

2. **Aprendizaje Autom√°tico**
   - Mejora continua basada en feedback
   - Detecci√≥n autom√°tica de temas
   - Generaci√≥n de res√∫menes ejecutivos

3. **Colaboraci√≥n**
   - Compartir fragmentos de documentos
   - Anotaciones colaborativas
   - Sistema de revisi√≥n por pares
## ü§ù Contribuciones

¬°Las contribuciones son bienvenidas! Por favor, lee nuestra [gu√≠a de contribuci√≥n](CONTRIBUTING.md) para m√°s detalles.

## üìÑ Licencia

Este proyecto est√° bajo la Licencia MIT. Ver el archivo [LICENSE](LICENSE) para m√°s detalles.

## üìû Contacto

Para consultas o soporte, por favor contacta a [ron.h.castillo@gmail.com](mailto:ron.h.castillo@gmail.com)

---

<div align="center">
  Hecho con ‚ù§Ô∏è por Ronald Castillo Capino
</div>
- **Extracci√≥n de texto** con PyPDF2
- **Divisi√≥n inteligente** que mantiene el contexto
- **Metadatos** para seguimiento de fuentes

### üî¢ Generaci√≥n de Embeddings
- **Modelo**: `sentence-transformers/all-MiniLM-L6-v2`
- **Normalizaci√≥n**: Vectores unitarios para similitud coseno
- **Rendimiento**: Optimizado para b√∫squeda r√°pida

### üîç Recuperaci√≥n
- **Motor**: FAISS para b√∫squeda vectorial
- **Estrategia**: Top-5 fragmentos m√°s relevantes
- **Rendimiento**: Respuestas en tiempo real

### ü§ñ Generaci√≥n
- **Modelo Local**: `llama-3.1-nemotron-nano-8b-v1`
- **T√©cnicas Avanzadas**:
  - Prompt engineering
  - Control de contexto
  - Verificaci√≥n de hechos

## üìà Resultados Esperados

El sistema est√° dise√±ado para superar los siguientes umbrales:

| M√©trica | Objetivo | Explicaci√≥n |
|---------|----------|-------------|
| üéØ Fidelidad | > 0.90 | Las respuestas se basan estrictamente en el contexto |
| üéØ Relevancia | > 0.85 | Las respuestas responden directamente a la pregunta |
| üéØ Precisi√≥n | > 0.80 | Los fragmentos recuperados son relevantes |
| üéØ Recall | > 0.75 | Se recupera la mayor√≠a de la informaci√≥n relevante |

üí° Estos valores pueden variar seg√∫n la calidad del documento fuente y la complejidad de las preguntas.

## üö® Soluci√≥n de Problemas

### Problemas Comunes y Soluciones:

1. **üîë Error de Autenticaci√≥n**
   ```
   [ERROR] Error de autenticaci√≥n con el modelo
   ```
   üîß **Soluci√≥n**: Verifica que LM Studio est√© ejecut√°ndose y que la URL en `.env` sea correcta

2. **üìÑ Archivo PDF no encontrado**
   ```
   [ERROR] No se encontr√≥ el archivo PDF
   ```
   üîß **Soluci√≥n**: Aseg√∫rate de que el archivo existe en `data/PDF-GenAI-Challenge.pdf`

3. **üêå Rendimiento lento**
   ```
   [INFO] La generaci√≥n de respuestas est√° tardando m√°s de lo esperado
   ```
   üîß **Soluci√≥n**:
   - Reduce el n√∫mero de fragmentos recuperados
   - Usa un modelo m√°s peque√±o
   - Verifica el rendimiento de tu hardware

4. **üíæ Problemas de memoria**
   ```
   MemoryError: No se puede asignar memoria
   ```
   üîß **Soluci√≥n**:
   - Reduce el tama√±o del lote de procesamiento
   - Cierra otras aplicaciones que consuman mucha memoria
   - Considera usar un equipo con m√°s RAM

## üöÄ Pr√≥ximos Pasos

### Mejoras Planificadas:

1. **üéØ Mejora de Precisi√≥n**
   - Implementar re-ranking de resultados
   - A√±adir verificaci√≥n cruzada de hechos
   - Mejorar la recuperaci√≥n de contexto

2. **‚ö° Rendimiento**
   - Optimizar el uso de memoria
   - Implementar cach√© de embeddings
   - Soporte para procesamiento por lotes

3. **üåê Interfaz de Usuario**
   - Desarrollar interfaz web interactiva
   - A√±adir visualizaci√≥n de fuentes
   - Soporte para m√∫ltiples formatos de documentos

4. **üìà Escalabilidad**
   - Soporte para m√∫ltiples documentos
   - B√∫squeda distribuida
   - Indexaci√≥n incremental

## üìû Soporte

### ¬øNecesitas ayuda?

1. **üìã Verifica los logs** en la consola para mensajes de error detallados
2. **üìä Revisa el reporte** en `evaluate/evaluation_report.txt`
3. **üîç Comprueba** la configuraci√≥n en `.env`
4. **üìö Consulta la documentaci√≥n** de las dependencias

### ¬øSigues teniendo problemas?

- üìß Env√≠a un correo a [ron.h.castillo@gmail.com](mailto:ron.h.castillo@gmail.com)
- üîó Incluye los mensajes de error y los pasos para reproducir el problema

---

‚ú® **Desarrollado con ‚ù§Ô∏è para el Desaf√≠o de Ingeniero de IA** ‚ú®

---

### üìö Recursos Adicionales

- [Documentaci√≥n de RAGAS](https://github.com/explodinggradients/ragas)
- [Gu√≠a de FAISS](https://github.com/facebookresearch/faiss)
- [Documentaci√≥n de LM Studio](https://lmstudio.ai/docs/)

üéØ **Objetivo del Proyecto**: Crear un sistema de preguntas y respuestas confiable y escalable para documentaci√≥n t√©cnica.