# ğŸ§ âœ¨ Sistema RAG Avanzado: Tu Experto en Aprendizaje EstadÃ­stico

<div align="center">
  <img src="https://img.shields.io/badge/Python-3.10.18-306998?style=for-the-badge&logo=python&logoColor=white" alt="Python 3.10.18">
  <img src="https://img.shields.io/badge/LangChain-0.1.20-FF6B6B?style=for-the-badge&logo=python&logoColor=white" alt="LangChain">
  <img src="https://img.shields.io/badge/FAISS-Vector%20Store-00C4CC?style=for-the-badge&logo=facebook&logoColor=white" alt="FAISS">
  <img src="https://img.shields.io/badge/LLM-Llama--3.1-FF9900?style=for-the-badge&logo=llama&logoColor=white" alt="LLM Model">
  <img src="https://img.shields.io/badge/GPU-Accelerated-76B900?style=for-the-badge&logo=nvidia&logoColor=white" alt="GPU Accelerated">
</div>

<div align="center">
  <h3>ğŸš€ Transforma documentos en conocimiento accionable con IA</h3>
  <p>Una soluciÃ³n todo-en-uno para extraer, procesar y generar conocimiento a partir de documentos tÃ©cnicos</p>
</div>

ğŸ‘¨â€ğŸ’» **Autor**: Ronald Castillo Capino  
ğŸ“§ **Contacto**: [ron.h.castillo@gmail.com](mailto:ron.h.castillo@gmail.com)

> ğŸ’¡ Este proyecto implementa un sistema avanzado de Preguntas y Respuestas (Q&A) que combina recuperaciÃ³n de informaciÃ³n con generaciÃ³n de lenguaje natural, garantizando respuestas precisas, verificables y basadas en documentos especÃ­ficos.

## ğŸ Compatibilidad con Python 3.10.18

Este proyecto estÃ¡ desarrollado y probado especÃ­ficamente con Python 3.10.18. La elecciÃ³n de esta versiÃ³n se debe a:

- ğŸš€ **Rendimiento optimizado** para operaciones de procesamiento de lenguaje natural
- ğŸ”’ **Estabilidad** en el ecosistema de IA/ML
- ğŸ“¦ **Compatibilidad** con bibliotecas clave como PyTorch y Transformers

### Verifica tu versiÃ³n de Python

```bash
# Verificar versiÃ³n instalada
python --version
# DeberÃ­as ver: Python 3.10.18

# O alternativamente
python -c "import sys; print(f'Python {sys.version}')"
```

### ConfiguraciÃ³n recomendada para entornos virtuales

```bash
# Crear entorno virtual con Python 3.10.18
py -3.10 -m venv venv  # Windows
# o
python3.10 -m venv venv  # Linux/Mac

# Activar el entorno
.\venv\Scripts\activate  # Windows
source venv/bin/activate  # Linux/Mac
```

## ğŸŒŸ Â¿QuÃ© hace este proyecto?

Imagina tener un asistente experto que puede leer y entender documentos tÃ©cnicos complejos, y responder a tus preguntas con precisiÃ³n. Â¡Eso es exactamente lo que ofrece este sistema!

### ğŸ¯ CaracterÃ­sticas principales

| CaracterÃ­stica | DescripciÃ³n | Beneficio |
|----------------|-------------|-----------|
| ğŸ” **BÃºsqueda SemÃ¡ntica** | Entiende el significado detrÃ¡s de las palabras | Encuentra informaciÃ³n relevante incluso con tÃ©rminos diferentes |
| ğŸ§  **GeneraciÃ³n Contextual** | Responde usando el modelo Llama-3.1 | Respuestas naturales y precisas con citas a las fuentes |
| ğŸ“Š **AutoevaluaciÃ³n** | Mide la calidad con mÃ©tricas RAGAS | Confianza en la precisiÃ³n de las respuestas |
| âš¡ **Rendimiento Ã“ptimo** | Optimizado para GPU NVIDIA | Respuestas rÃ¡pidas incluso con documentos extensos |

### ğŸ“ Casos de Uso

- ğŸ“š **Estudiantes**: Entiende conceptos complejos de aprendizaje estadÃ­stico
- ğŸ‘¨â€ğŸ’» **Desarrolladores**: Integra conocimiento tÃ©cnico en tus aplicaciones
- ğŸ”¬ **Investigadores**: Analiza y extrae informaciÃ³n de papers acadÃ©micos
- ğŸ¢ **Empresas**: Crea una base de conocimiento interna accesible

### ğŸ—ï¸ CÃ³mo funciona

```mermaid
flowchart LR
    A[ğŸ“„ Documento PDF] --> B[ğŸ” Procesamiento]
    B --> C[ğŸ“š Base de Conocimiento]
    D[â“ Pregunta] --> E[ğŸ§  Modelo RAG]
    C --> E
    E --> F[ğŸ’¡ Respuesta con Fuentes]
```

ğŸ’¡ **Caso de Uso Principal**: Sistema experto de preguntas y respuestas sobre el libro "An Introduction to Statistical Learning with Applications in Python", permitiendo a los usuarios obtener explicaciones claras y precisas sobre conceptos de aprendizaje estadÃ­stico.

### ğŸš€ DesempeÃ±o con AceleraciÃ³n por GPU

El sistema estÃ¡ optimizado para aprovechar al mÃ¡ximo la capacidad de procesamiento en paralelo de tu GPU local, ofreciendo tiempos de respuesta rÃ¡pidos con el modelo Llama-3.1-Nemotron-Nano-8B. La configuraciÃ³n incluye:

- **Contexto extendido**: 16,384 tokens
- **Capas GPU**: 30 capas (ajustable segÃºn VRAM)
- **Procesamiento por lotes**: 512 tokens
- **Hilos de CPU**: 8 hilos

### ğŸ”§ Requisitos de Hardware

| Componente | MÃ­nimo | Recomendado |
|------------|--------|-------------|
| **GPU** | NVIDIA RTX 3060 | NVIDIA RTX 4090 o superior |
| **VRAM** | 12GB | 24GB+ |
| **RAM del Sistema** | 32GB | 64GB+ |
| **Almacenamiento** | 15GB libres | SSD NVMe |

### âš™ï¸ ConfiguraciÃ³n Ã“ptima

1. **Controladores NVIDIA**
   ```bash
   # Verificar instalaciÃ³n de controladores
   nvidia-smi
   # VersiÃ³n mÃ­nima recomendada: 525.60.13
   ```

2. **Bibliotecas CUDA**
   - CUDA Toolkit 11.7 o superior
   - cuDNN 8.5 o superior
   - Verificar instalaciÃ³n:
     ```bash
     nvcc --version
     ```

### ğŸš€ Optimizaciones Implementadas

- **Inferencia Acelerada por GPU**
  - Todas las operaciones del modelo se ejecutan en la GPU
  - Soporte para CUDA y cuBLAS para operaciones matriciales

- **GestiÃ³n Eficiente de Memoria**
  - Carga selectiva de capas del modelo
  - OptimizaciÃ³n de memoria intermedia
  - Soporte para precisiÃ³n mixta (FP16/FP32)

- **Procesamiento por Lotes**
  - Procesamiento paralelo de mÃºltiples consultas
  - Ajuste automÃ¡tico del tamaÃ±o de lote segÃºn la VRAM disponible

### ğŸ“Š Rendimiento Esperado

| ConfiguraciÃ³n | Tokens/seg | Memoria GPU |
|---------------|------------|-------------|
| RTX 3060 (12GB) | 18-25 | ~12GB |
| RTX 3090 (24GB) | 30-40 | ~22GB |
| RTX 4090 (24GB) | 45-60 | ~24GB |

*Nota: El rendimiento puede variar segÃºn la carga del sistema y la configuraciÃ³n especÃ­fica.*

### CaracterÃ­sticas Clave

- âœ… **Respuestas Basadas en Contexto**: Cada respuesta estÃ¡ respaldada por fragmentos especÃ­ficos del documento
- ğŸ” **BÃºsqueda SemÃ¡ntica**: Encuentra informaciÃ³n relevante incluso con consultas en lenguaje natural
- ğŸ“ˆ **EvaluaciÃ³n Continua**: Sistema integrado para medir y mejorar la calidad de las respuestas
- ğŸš€ **Rendimiento Optimizado**: DiseÃ±ado para funcionar eficientemente en hardware estÃ¡ndar

## ğŸ¯ Objetivo

Desarrollar un asistente de IA que:

âœ… Proporcione respuestas precisas basadas en documentos especÃ­ficos  
âœ… Mantenga la trazabilidad de las fuentes de informaciÃ³n  
âœ… EvalÃºe automÃ¡ticamente la calidad de las respuestas  
âœ… Sea fÃ¡cil de implementar y mantener

## ğŸ“‚ Estructura del CÃ³digo

El proyecto estÃ¡ organizado en los siguientes archivos principales:

### 1. `main.py`
MÃ³dulo principal que implementa el sistema RAG con las siguientes caracterÃ­sticas:

- **Clase Principal**: `RAGSystem`
  - Procesa documentos PDF y crea un Ã­ndice de bÃºsqueda semÃ¡ntica
  - Implementa bÃºsqueda vectorial usando FAISS
  - Genera respuestas utilizando un modelo de lenguaje local

- **ConfiguraciÃ³n**:
  ```python
  # Modelo por defecto
  MODEL_NAME = 'mistral-7b-instruct-v0.2'
  
  # ConfiguraciÃ³n de fragmentaciÃ³n
  CHUNK_SIZE = 4000  # TamaÃ±o de fragmentos de texto
  CHUNK_OVERLAP = 200  # Solapamiento entre fragmentos
  TOP_K_RETRIEVAL = 5  # NÃºmero de fragmentos a recuperar
  
  # ConfiguraciÃ³n del modelo de lenguaje
  LLM_TEMPERATURE = 0.1
  MAX_TOKENS = 512
  ```

### 2. `evaluate.py`
MÃ³dulo para evaluar el rendimiento del sistema RAG con las siguientes caracterÃ­sticas:

- **Funcionalidades**:
  - Carga preguntas y respuestas de referencia desde `faq.json`
  - Implementa evaluaciÃ³n con RAGAS (Retrieval-Augmented Generation Assessment)
  - Soporta modelos locales para embeddings y generaciÃ³n
  - Genera reportes detallados de evaluaciÃ³n

- **MÃ©tricas implementadas**:
  - `Faithfulness`: Mide quÃ© tan fiel es la respuesta al contexto proporcionado
  - `Answer Relevancy`: EvalÃºa la relevancia de la respuesta respecto a la pregunta
  - `Context Precision`: Mide la precisiÃ³n del contexto recuperado
  - `Context Recall`: EvalÃºa quÃ© tan bien se recupera la informaciÃ³n relevante

- **ConfiguraciÃ³n**:
  ```python
  # ConfiguraciÃ³n de evaluaciÃ³n
  EVAL_SAMPLES = 2  # NÃºmero de ejemplos a evaluar
  LLM_TEMPERATURE = 0.1  # Controla la aleatoriedad de las respuestas
  MAX_TOKENS = 512  # MÃ¡ximo nÃºmero de tokens por respuesta
  
  # ConfiguraciÃ³n de modelos locales
  LOCAL_LLM_URL = "http://localhost:1234/v1"  # Endpoint del modelo local
  EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
  ```

### 3. `utils.py`
MÃ³dulo de utilidades para el procesamiento de texto y gestiÃ³n de vectores:

- **Funciones principales**:
  - `load_and_process_pdf()`: Extrae y procesa texto de archivos PDF
  - `chunk_text_semantically()`: Divide el texto en fragmentos significativos con solapamiento
  - `create_vector_store()`: Crea y gestiona el almacÃ©n vectorial FAISS

- **CaracterÃ­sticas**:
  - Soporte para mÃºltiples formatos de documentos
  - TokenizaciÃ³n inteligente que preserva la estructura semÃ¡ntica
  - IntegraciÃ³n con modelos de embeddings de Hugging Face
  - GestiÃ³n eficiente de memoria para documentos grandes

## ğŸ—ï¸ Arquitectura del Sistema

```mermaid
graph TD
    A[Usuario] -->|Pregunta| B[Procesamiento de Texto]
    B --> C[GeneraciÃ³n de Embeddings]
    C --> D[Base de Vectores FAISS]
    D -->|Contexto Relevante| E[Modelo de Lenguaje]
    E -->|Respuesta| A
    F[Documentos] -->|Procesamiento| G[Base de Conocimiento]
    G --> D
    H[MÃ³dulo de EvaluaciÃ³n] <-->|MÃ©tricas| E
```

### ğŸ”§ Componentes Principales

#### 1. **Procesamiento de Documentos**
- **ExtracciÃ³n de Texto**: Utiliza PyPDF2 para extraer texto de documentos PDF
- **Limpieza de Texto**: EliminaciÃ³n de caracteres especiales, normalizaciÃ³n de espacios
- **TokenizaciÃ³n**: DivisiÃ³n del texto en unidades significativas
- **FragmentaciÃ³n SemÃ¡ntica**:
  - TamaÃ±o de fragmento: 4000 caracteres
  - Solapamiento: 200 caracteres
  - PreservaciÃ³n de contexto entre fragmentos

#### 2. **Modelo de Embeddings**
- **Modelo**: `sentence-transformers/all-MiniLM-L6-v2`
- **DimensiÃ³n de Embeddings**: 384
- **NormalizaciÃ³n**: Vectores unitarios para similitud coseno
- **Rendimiento**: Optimizado para equilibrio entre precisiÃ³n y velocidad

#### 3. **Almacenamiento Vectorial (FAISS)**
- **Ãndice**: `IndexFlatL2` para bÃºsqueda exacta
- **MÃ©tricas**: Distancia euclidiana (L2)
- **Optimizaciones**:
  - BÃºsqueda por lotes
  - Filtrado por umbral de similitud
  - RecuperaciÃ³n de los 5 mejores resultados

#### 4. **Modelo de Lenguaje Local**
- **Modelo Base**: `Llama-3.1-Nemotron-Nano-8B-v1`
- **EjecuciÃ³n**:
  - Ejecutado localmente a travÃ©s de LM Studio
  - AceleraciÃ³n por GPU para mÃ¡ximo rendimiento
  - VersiÃ³n cuantizada (Q4_K_S) para eficiencia
- **ConfiguraciÃ³n**:
  - Temperatura: 0.1 (para respuestas deterministas)
  - Tokens mÃ¡ximos: 2048
  - Contexto: 16,384 tokens
  - Capas GPU: 30 (ajustable segÃºn VRAM)
  - Procesamiento por lotes: 512 tokens
  - Hilos CPU: 8
- **Prompt Engineering**:
  - Instrucciones claras para el modelo
  - Formato estructurado de respuestas
  - Manejo de incertidumbre

#### 5. **MÃ³dulo de EvaluaciÃ³n**
- **MÃ©tricas RAGAS**:
  - Faithfulness (Fidelidad)
  - Answer Relevancy (Relevancia de la respuesta)
  - Context Precision (PrecisiÃ³n del contexto)
  - Context Recall (RecuperaciÃ³n del contexto)
  - Answer Similarity (Similitud de respuestas)
  - Answer Correctness (CorrecciÃ³n de respuestas)
  - Harmfulness (Contenido daÃ±ino)
- **Reportes**:
  - Salida detallada en consola
  - Archivo de registro estructurado
  - EstadÃ­sticas agregadas

#### 6. **API y Servicios**
- **Interfaz de LÃ­nea de Comandos (CLI)**
- **API RESTful** (opcional)
- **IntegraciÃ³n con LM Studio**
  - Endpoint: `http://localhost:1234/v1`
  - Soporte para streaming
  - Manejo de tiempo de espera

### ğŸ”„ Flujo de Datos

1. **Ingreso de Consulta**
   - El usuario ingresa una pregunta en lenguaje natural
   - La consulta se normaliza y procesa

2. **BÃºsqueda SemÃ¡ntica**
   - La consulta se convierte en embedding usando `sentence-transformers/all-MiniLM-L6-v2`
   - Se buscan los fragmentos mÃ¡s similares en el Ã­ndice FAISS
   - Se recuperan los 5 fragmentos mÃ¡s relevantes

3. **GeneraciÃ³n de Respuesta**
   - Los fragmentos recuperados se combinan con un prompt estructurado
   - El modelo de lenguaje local (por defecto Mistral 7B) genera una respuesta contextualizada
   - Se incluyen referencias a las pÃ¡ginas del documento original

4. **EvaluaciÃ³n de Calidad**
   - El sistema calcula mÃ©tricas de evaluaciÃ³n automÃ¡tica
   - Se genera un reporte detallado del rendimiento
   - Las interacciones se registran para anÃ¡lisis posterior

## ğŸš€ Comenzando en 3, 2, 1...

### ğŸ“¥ Requisitos Previos

- Python 3.10.18 ([Descargar](https://www.python.org/downloads/release/python-31018/))
- Git
- CUDA Toolkit (para aceleraciÃ³n GPU)
- pip (gestor de paquetes de Python)

### âš™ï¸ ConfiguraciÃ³n del Entorno (.env)

El archivo `.env` es fundamental para el funcionamiento del sistema. A continuaciÃ³n se detallan todas las configuraciones disponibles:

### ConfiguraciÃ³n BÃ¡sica
```ini
# ===== ConfiguraciÃ³n del Servidor Local =====
LOCAL_LLM_URL=http://192.168.100.5:1234/v1  # URL del servidor LM Studio
MODEL_NAME=mistral-7b-instruct-v0.2          # Nombre del modelo a utilizar
```

### Rutas de Archivos
```ini
# Ruta al modelo GGUF (descargado automÃ¡ticamente si no existe)
LOCAL_MODEL_PATH="C:\\Users\\ronal\\.cache\\lm-studio\\models\\jonahhenry\\mistral-7b-instruct-v0.2.Q4_K_M-GGUF\\mistral-7b-instruct-v0.2.Q4_K_M.gguf"
PDF_PATH=data/PDF-GenAI-Challenge.pdf  # Ruta al documento PDF de entrada
CACHE_DIR=./cache                     # Directorio para cachÃ©
```

### ConfiguraciÃ³n del Modelo
```ini
# ===== ConfiguraciÃ³n de GeneraciÃ³n =====
LLM_TEMPERATURE=0.1      # Controla la creatividad (0-1, mÃ¡s bajo = mÃ¡s determinista)
TOP_P=0.9                # Muestreo de nÃºcleo (nucleus sampling)
MAX_TOKENS=128           # Longitud mÃ¡xima de las respuestas
FREQUENCY_PENALTY=0.0    # PenalizaciÃ³n por repeticiÃ³n de tokens
PRESENCE_PENALTY=0.6     # PenalizaciÃ³n por repeticiÃ³n de temas
```

### Rendimiento y Recursos
```ini
# ===== ConfiguraciÃ³n de Rendimiento =====
N_CTX=2048               # TamaÃ±o del contexto
N_BATCH=1                # TamaÃ±o del lote para inferencia
N_GPU_LAYERS=0           # Capas a ejecutar en GPU (0 = CPU, 99 = todas en GPU)
N_THREADS=4              # Hilos de CPU a utilizar
```

### Procesamiento de Documentos
```ini
# ===== ConfiguraciÃ³n de FragmentaciÃ³n =====
CHUNK_SIZE=4000          # TamaÃ±o de los fragmentos de texto
CHUNK_OVERLAP=200        # Solapamiento entre fragmentos
TOP_K_RETRIEVAL=5        # NÃºmero de fragmentos a recuperar
SIMILARITY_THRESHOLD=0.7 # Umbral de similitud mÃ­nimo
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
```

### EvaluaciÃ³n
```ini
# ===== ConfiguraciÃ³n de EvaluaciÃ³n =====
EVAL_SAMPLES=0           # NÃºmero de ejemplos a evaluar (0 = todos)
EVALUATION_INPUT_PATH=evaluate/faq.json
EVALUATION_OUTPUT_PATH=evaluate/evaluation_report.txt
```

### Registro (Logging)
```ini
# ===== ConfiguraciÃ³n de Logs =====
LOG_LEVEL=INFO           # Nivel de detalle del registro
LOG_FILE=rag_system.log  # Archivo de registro

# ConfiguraciÃ³n de entorno
GIT_PYTHON_REFRESH=quiet
TRANSFORMERS_OFFLINE=1   # Modo offline para transformers
RAGAS_DO_NOT_TRACK=true  # Deshabilitar telemetrÃ­a de RAGAS
```

## âš™ï¸ InstalaciÃ³n Paso a Paso

```bash
# 1. Clona el repositorio
ğŸš git clone https://github.com/tu-usuario/tu-proyecto.git
ğŸ“‚ cd tu-proyecto

# 2. Crea y activa el entorno virtual
ğŸ python -m venv venv
# En Windows:
ğŸ”Œ .\venv\Scripts\activate
# En Linux/Mac:
# ğŸ”Œ source venv/bin/activate

# 3. Instala las dependencias
ğŸ“¦ pip install --upgrade pip
ğŸ“¦ pip install -r requirements.txt

# 4. Configura las variables de entorno
ğŸ”§ copy .env.example .env  # Windows
# En Linux/Mac:
# ğŸ”§ cp .env.example .env

# 5. Edita el archivo .env con tus configuraciones
âš™ï¸ notepad .env  # O usa tu editor favorito
```

### ğŸ® Uso BÃ¡sico

```python
# ğŸ“‚ main.py
from __future__ import annotations  # Para mejor compatibilidad de tipos
import os
import sys
from pathlib import Path
from typing import Dict, Any

# Verificar versiÃ³n de Python
if sys.version_info < (3, 10):
    raise RuntimeError("Se requiere Python 3.10.18 o superior")

# ConfiguraciÃ³n de rutas
PROJECT_ROOT = Path(__file__).parent
sys.path.append(str(PROJECT_ROOT))

# Cargar variables de entorno
from dotenv import load_dotenv
load_dotenv()

# Importaciones locales
from main import RAGSystem

# Cargar variables de entorno
load_dotenv()

# ğŸ—ï¸ Inicializa el sistema con tu documento
print("ğŸš€ Inicializando el sistema RAG...")
rag = RAGSystem("documentos/libro_estadistica.pdf")
rag.setup_system()  # â³ Esto puede tomar unos minutos la primera vez

# â“ Ejemplo de pregunta
pregunta = "Â¿CÃ³mo funciona la regresiÃ³n logÃ­stica?"
print(f"\nğŸ” Procesando pregunta: {pregunta}")

# ğŸ¯ Generar respuesta
respuesta = rag.generate_response(
    pregunta,
    temperature=0.1,  # Controla la creatividad (0-1)
    max_tokens=512    # Longitud mÃ¡xima de la respuesta
)

# âœ¨ Mostrar resultados
print("\n" + "="*80)
print(f"ğŸ” Pregunta: {pregunta}")
print("-"*80)
print(f"ğŸ’¡ Respuesta: {respuesta['answer']}")
print("-"*80)
print(f"ğŸ“š Fuentes: {', '.join(respuesta['sources'])}")
print(f"ğŸ¯ Confianza: {respuesta['confidence']:.1%}")
print("="*80 + "\n")
```

### ğŸ¯ Ejemplos PrÃ¡cticos con Tipado EstÃ¡tico

#### 1. BÃºsqueda de Conceptos con Tipado
```python
def obtener_explicacion(rag: RAGSystem, concepto: str, temp: float = 0.7) -> Dict[str, Any]:
    """Obtiene una explicaciÃ³n detallada de un concepto estadÃ­stico.
    
    Args:
        rag: Instancia de RAGSystem
        concepto: TÃ©rmino o concepto a explicar
        temp: Temperatura para la generaciÃ³n (0-1)
        
    Returns:
        Dict con la respuesta y metadatos
    """
    respuesta = rag.generate_response(
        f"Explica el concepto de {concepto} con un ejemplo prÃ¡ctico",
        temperature=min(max(temp, 0), 1),  # Asegurar valor entre 0 y 1
        max_tokens=512
    )
    return respuesta

# Uso
explicacion = obtener_explicacion(rag, "teorema de Bayes", temp=0.7)
print(f"ğŸ“ {explicacion['answer']}")
print(f"ğŸ“Š Confianza: {explicacion['confidence']:.1%}")
```

#### 2. ComparaciÃ³n de Algoritmos
```python
respuesta = rag.generate_response(
    "Compara los Ã¡rboles de decisiÃ³n con los bosques aleatorios",
    max_tokens=1000  # LÃ­mite de longitud
)
```

#### 3. GeneraciÃ³n de Ejemplos de CÃ³digo
```python
respuesta = rag.generate_response(
    "Muestra un ejemplo de implementaciÃ³n de regresiÃ³n lineal en Python",
    include_code=True
)
```

### 2. EvaluaciÃ³n del Rendimiento
```python
from evaluate import run_ragas_evaluation, create_evaluation_dataset
from datasets import Dataset

# Cargar datos de evaluaciÃ³n
eval_data = create_evaluation_dataset()

# Ejecutar evaluaciÃ³n
results = run_ragas_evaluation(eval_data)

# Mostrar resultados
print("\n=== Resultados de la EvaluaciÃ³n ===")
for metric, score in results.items():
    print(f"{metric}: {score:.4f}")
```

### 3. ConfiguraciÃ³n Avanzada
```python
# Personalizar la evaluaciÃ³n
os.environ["RAGAS_EMBEDDINGS"] = "huggingface"
os.environ["RAGAS_DO_NOT_TRACK"] = "true"

# Usar un modelo local diferente
os.environ["LOCAL_LLM_URL"] = "http://localhost:1234/v1"

# Configurar el nivel de detalle del logging
import logging
logging.basicConfig(level=logging.INFO)
```

## âš™ï¸ ConfiguraciÃ³n Avanzada

### ğŸ”§ Variables de Entorno Clave

Crea un archivo `.env` en la raÃ­z del proyecto con estas configuraciones:

```ini
# ğŸ¯ ConfiguraciÃ³n del Modelo
MODEL_NAME=llama-3.1-nemotron-nano-8b-v1
LOCAL_MODEL_PATH=modelos/llama-3.1.gguf  # Ruta a tu modelo

# âš¡ Rendimiento
GPU_LAYERS=30  # Capas a cargar en GPU (ajustar segÃºn VRAM)
BATCH_SIZE=512  # TamaÃ±o de lote para inferencia
THREADS=8      # Hilos de CPU para procesamiento

# ğŸ” BÃºsqueda
TOP_K_RESULTS=5       # NÃºmero de fragmentos a recuperar
SIMILARITY_THRESHOLD=0.7  # Umbral de similitud mÃ­nimo

# ğŸ“Š EvaluaciÃ³n
EVALUATION_SAMPLES=10  # NÃºmero de ejemplos para evaluaciÃ³n
EVAL_TEMPERATURE=0.1   # Temperatura para generaciÃ³n en evaluaciÃ³n
```

### ğŸ› ï¸ PersonalizaciÃ³n Avanzada

#### 1. Ajuste de ParÃ¡metros del Modelo
```python
# ğŸ“‚ main.py
rag = RAGSystem(
    "documento.pdf",
    model_params={
        'temperature': 0.3,  # Controla la creatividad (0-1)
        'max_tokens': 1024,  # Longitud mÃ¡xima de respuesta
        'top_p': 0.9,       # Muestreo de nÃºcleo
        'repeat_penalty': 1.1  # PenalizaciÃ³n por repeticiÃ³n
    }
)
```

#### 2. PersonalizaciÃ³n del Prompt
Puedes modificar el prompt del sistema para adaptarlo a tus necesidades:

```python
# ğŸ“‚ prompts/custom_prompt.txt
Eres un experto en aprendizaje estadÃ­stico. Responde de manera clara y concisa.

Contexto:
{context}

Pregunta: {question}

Respuesta (incluye las pÃ¡ginas de referencia [p.XX]):
```

Luego cÃ¡rgalo asÃ­:
```python
rag = RAGSystem("documento.pdf", prompt_file="prompts/custom_prompt.txt")
```

# ConfiguraciÃ³n de Rendimiento
N_CTX=16384
N_GPU_LAYERS=30
N_BATCH=512
N_THREADS=8

# ConfiguraciÃ³n de GeneraciÃ³n
LLM_TEMPERATURE=0.1
MAX_TOKENS=2048

# ConfiguraciÃ³n de FragmentaciÃ³n
CHUNK_SIZE=4000
CHUNK_OVERLAP=200
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# ConfiguraciÃ³n de RecuperaciÃ³n
TOP_K_RETRIEVAL=5
CONTEXT_WINDOW=131072
```

## ğŸ“ Estructura del Proyecto

```
.
â”œâ”€â”€ data/                           # Directorio de datos
â”‚   â””â”€â”€ PDF-GenAI-Challenge.pdf    # Documento fuente para el sistema RAG
â”‚
â”œâ”€â”€ evaluate/                      # Resultados de evaluaciÃ³n
â”‚   â””â”€â”€ evaluation_report.txt      # Reporte detallado de mÃ©tricas
â”‚
â”œâ”€â”€ .env                   # Plantilla de configuraciÃ³n
â”œâ”€â”€ main.py                       # Punto de entrada principal
â”œâ”€â”€ evaluate.py                   # MÃ³dulo de evaluaciÃ³n
â”œâ”€â”€ utils.py                      # Utilidades y helpers
â”œâ”€â”€ requirements.txt              # Dependencias del proyecto
â””â”€â”€ README.md                     # DocumentaciÃ³n
```

### ğŸ“‹ Archivos Principales

#### `main.py`
Punto de entrada principal que implementa:
- Carga y procesamiento de documentos
- Interfaz de lÃ­nea de comandos
- IntegraciÃ³n de componentes RAG
- Manejo de errores y logging

#### `evaluate.py`
MÃ³dulo de evaluaciÃ³n que incluye:
- ImplementaciÃ³n de mÃ©tricas RAGAS
- GeneraciÃ³n de reportes detallados
- Herramientas para anÃ¡lisis comparativo
- VisualizaciÃ³n de resultados

#### `utils.py`
Funciones auxiliares para:
- Procesamiento de texto y PDF
- Manejo de embeddings
- Utilidades de sistema
- ConfiguraciÃ³n y logging

#### `requirements.txt`
Lista de dependencias con versiones especÃ­ficas para garantizar compatibilidad.

#### `.env`
Archivo de configuraciÃ³n que debe contener:
- Rutas a modelos
- Configuraciones de ejecuciÃ³n
- ParÃ¡metros del sistema

## âš™ï¸ InstalaciÃ³n y ConfiguraciÃ³n

### 1. ğŸ› ï¸ Requisitos Previos

- Python 3.8 o superior
- pip (gestor de paquetes de Python)
- Git (opcional, para clonar el repositorio)
- LM Studio (para ejecutar modelos locales)
- Al menos 8GB de RAM (16GB recomendado)

### 2. ğŸ—ï¸ ConfiguraciÃ³n del Entorno

```bash
# 1. Clonar el repositorio (opcional)
git clone <repo-url>
cd rag_challenge

# 2. Crear y activar entorno virtual
python -m venv rag_env
source rag_env/bin/activate  # Linux/Mac
# o
rag_env\Scripts\activate     # Windows

# 3. Actualizar pip
python -m pip install --upgrade pip

# 4. Instalar dependencias
pip install -r requirements.txt
```

### 3. ğŸ” ConfiguraciÃ³n de Variables de Entorno

1. Copiar el archivo de ejemplo:
   ```bash
   cp .env.example .env
   ```

2. Editar el archivo `.env` con tus configuraciones:
   ```env
   # ===== ConfiguraciÃ³n del Modelo =====
   LOCAL_LLM_URL=http://localhost:1234/v1
   MODEL_NAME=llama-3.1-nemotron-nano-8b-v1
   
   # ===== ConfiguraciÃ³n de Embeddings =====
   EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
   
   # ===== ConfiguraciÃ³n de Procesamiento =====
   CHUNK_SIZE=4000
   CHUNK_OVERLAP=200
   TOP_K_RETRIEVAL=5
   
   # ===== ConfiguraciÃ³n del Modelo =====
   LLM_TEMPERATURE=0.1
   MAX_TOKENS=1024
   CONTEXT_WINDOW=2048
   
   # ===== ConfiguraciÃ³n de EvaluaciÃ³n =====
   EVAL_SAMPLES=5
   EVALUATION_OUTPUT_PATH=evaluate/evaluation_report.txt
   
   # ===== ConfiguraciÃ³n del Sistema =====
   LOG_LEVEL=INFO
   CACHE_DIR=./cache
   ```

### 4. ğŸ“¦ Dependencias Principales

El archivo `requirements.txt` contiene todas las dependencias necesarias:

```
# Procesamiento de documentos
PyPDF2>=3.0.0
python-dotenv>=1.0.0

# Procesamiento de lenguaje natural
sentence-transformers>=2.2.2
numpy>=1.24.0

# Almacenamiento vectorial
faiss-cpu>=1.7.4  # o faiss-gpu si tienes CUDA

# Modelo de lenguaje
llama-cpp-python>=0.2.0

# EvaluaciÃ³n
ragas>=0.0.21
datasets>=2.14.0

# Utilidades
tqdm>=4.65.0
colorama>=0.4.6
python-dotenv>=1.0.0
```

### 5. ğŸš€ ConfiguraciÃ³n del Modelo Local

#### Descarga e InstalaciÃ³n del Modelo

1. **Descargar el modelo**:
   - Nombre: `Llama-3.1-Nemotron-Nano-4B-v1.1-GGUF`
   - TamaÃ±o: ~2.5GB (versiÃ³n cuantizada Q4_K_M)
   - UbicaciÃ³n por defecto: `C:\Users\[usuario]\.cache\lm-studio\models\`

2. **ConfiguraciÃ³n en LM Studio**:
   - Abrir LM Studio y seleccionar "Download a model"
   - Buscar: `llama-3.1-nemotron-nano-8b-v1-GGUF`
   - Descargar la versiÃ³n `Q4_K_M` para el mejor equilibrio entre rendimiento y calidad

3. **Habilitar AceleraciÃ³n por GPU**:
   - Ir a ConfiguraciÃ³n â†’ Modelo
   - Seleccionar "Auto" o tu GPU especÃ­fica en "GPU Layers"
   - Activar "Use CUDA"
   - Establecer "Context Length" a 2048 tokens

4. **Iniciar el Servidor Local**:
   - Ir a la pestaÃ±a "Local Server"
   - Asegurarse de que "GPU Offload" estÃ© activado
   - Hacer clic en "Start Server"
   - Verificar que la URL sea `http://localhost:1234/v1`

5. **Verificar la ConfiguraciÃ³n de GPU**:
   - Abrir el Administrador de Tareas de Windows
   - Ir a la pestaÃ±a "Rendimiento"
   - Verificar que la GPU muestre actividad durante la inferencia
   - Confirmar que la memoria de GPU se estÃ© utilizando

#### ConfiguraciÃ³n Recomendada para Ã“ptimo Rendimiento

```env
# En tu archivo .env
LOCAL_LLM_URL=http://localhost:1234/v1
MODEL_NAME=llama-3.1-nemotron-nano-8b-v1
LLM_TEMPERATURE=0.1
MAX_TOKENS=1024
CONTEXT_WINDOW=2048
```

2. **Preparar los datos**:
   ```bash
   mkdir -p data
   cp /ruta/a/tu/documento.pdf data/PDF-GenAI-Challenge.pdf
   ```

3. **Verificar la instalaciÃ³n**:
   ```bash
   python -c "import torch; print(f'PyTorch: {torch.__version__}')"
   python -c "import transformers; print(f'Transformers: {transformers.__version__}')"
   ```

### 6. ğŸ§ª Prueba RÃ¡pida

```bash
# Ejecutar una consulta de prueba
echo "Â¿QuÃ© es el aprendizaje automÃ¡tico?" | python main.py

# Ejecutar evaluaciÃ³n bÃ¡sica
python evaluate.py --samples 3
```

### ğŸ“ Notas de InstalaciÃ³n

- Para mejor rendimiento, se recomienda usar una GPU compatible con CUDA
- El primer inicio puede tardar varios minutos mientras se descargan los modelos
- Verifica que el puerto 1234 estÃ© disponible para LM Studio

## ğŸ’» Uso del Sistema

### 1. Modos de OperaciÃ³n

#### Modo Interactivo
```bash
# Iniciar el sistema en modo interactivo
python main.py

# Ejemplo de sesiÃ³n:
Bienvenido al Sistema RAG de Control de Calidad
> Â¿Sobre quÃ© tema te gustarÃ­a consultar?
```

#### Modo por LÃ­nea de Comandos
```bash
# Hacer una pregunta especÃ­fica
python main.py --pregunta "Â¿QuÃ© es el trade-off entre sesgo y varianza?"

# Procesar un archivo con mÃºltiples preguntas
python main.py --archivo preguntas.txt --salida respuestas.json

# Opciones adicionales
python main.py \
  --pregunta "Explique la regresiÃ³n lineal" \
  --temperatura 0.3 \
  --max-tokens 500 \
  --mostrar-fuentes
```

### 2. Ejemplos de Preguntas

#### Conceptos BÃ¡sicos
```
- "Â¿QuÃ© es el aprendizaje supervisado?"
- "Explique el concepto de validaciÃ³n cruzada"
- "Â¿CuÃ¡les son las ventajas de los Ã¡rboles de decisiÃ³n?"
```

#### Comparaciones
```
- "Compare ridge y lasso regression"
- "Diferencia entre bagging y boosting"
- "Ventajas de SVM sobre regresiÃ³n logÃ­stica"
```

#### Aplicaciones PrÃ¡cticas
```
- "Â¿CÃ³mo manejar datos faltantes en un dataset?"
- "TÃ©cnicas para tratar el desbalance de clases"
- "MÃ©todos de selecciÃ³n de caracterÃ­sticas"
```

### 3. Opciones Avanzadas

#### ConfiguraciÃ³n de BÃºsqueda
```bash
# Ajustar el nÃºmero de fragmentos recuperados
python main.py --top-k 3

# Cambiar el umbral de similitud (0-1)
python main.py --umbral-similitud 0.75
```

#### Control de Salida
```bash
# Mostrar solo la respuesta sin fuentes
python main.py --pregunta "..." --formato simple

# Generar salida en formato JSON
python main.py --pregunta "..." --formato json

# Guardar resultados en un archivo
python main.py --pregunta "..." --salida resultado.txt
```

## ğŸ“Š EvaluaciÃ³n del Sistema

### 1. MÃ©tricas Implementadas

El sistema utiliza el framework RAGAS para evaluar la calidad de las respuestas con las siguientes mÃ©tricas:

| MÃ©trica | Rango Ã“ptimo | DescripciÃ³n |
|---------|--------------|-------------|
| **Faithfulness** | 0.8 - 1.0 | Mide si la respuesta se basa Ãºnicamente en el contexto |
| **Answer Relevancy** | > 0.7 | EvalÃºa la relevancia de la respuesta |
| **Context Precision** | > 0.6 | PrecisiÃ³n de los fragmentos recuperados |
| **Context Recall** | > 0.7 | Capacidad de recuperar informaciÃ³n relevante |
| **Answer Similarity** | > 0.75 | ComparaciÃ³n con respuestas de referencia |
| **Answer Correctness** | > 0.8 | PrecisiÃ³n fÃ¡ctica de la respuesta |
| **Harmfulness** | < 0.2 | DetecciÃ³n de contenido potencialmente daÃ±ino |

### 2. EjecuciÃ³n de la EvaluaciÃ³n

#### EvaluaciÃ³n BÃ¡sica
```bash
# Evaluar con 5 ejemplos (valor por defecto)
python evaluate.py
```

#### EvaluaciÃ³n Personalizada
```bash
# Especificar nÃºmero de muestras
python evaluate.py --muestras 10

# Evaluar mÃ©tricas especÃ­ficas
python evaluate.py --metricas fidelidad relevancia

# Generar reporte en formato JSON
python evaluate.py --formato json --salida resultados.json
```

#### EvaluaciÃ³n con Conjunto de Datos Personalizado
```bash
# Usar un archivo JSON con preguntas y respuestas de referencia
python evaluate.py --dataset datos_evaluacion.json
```

### 3. InterpretaciÃ³n de Resultados

#### Ejemplo de Salida
```
========================================
   RESULTADOS DE LA EVALUACIÃ“N RAGAS   
========================================

- Faithfulness: 0.87
  âœ“ Excelente: La respuesta se basa completamente en el contexto

- Answer Relevancy: 0.82
  âœ“ Muy buena: La respuesta es altamente relevante a la pregunta

- Context Precision: 0.75
  âœ“ Buena: La mayorÃ­a de los fragmentos recuperados son relevantes

- Context Recall: 0.68
  âœ“ Aceptable: Se recupera la mayor parte de la informaciÃ³n relevante

- Answer Similarity: 0.79
  âœ“ Buena: La respuesta es similar a la referencia esperada

- Answer Correctness: 0.83
  âœ“ Muy buena: La informaciÃ³n proporcionada es correcta

- Harmfulness: 0.05
  âœ“ Seguro: No se detectÃ³ contenido daÃ±ino

----------------------------------------
PuntuaciÃ³n Promedio: 0.79
Estado General: Buen rendimiento
----------------------------------------

ğŸ“ Recomendaciones:
- Mejorar la recuperaciÃ³n de contexto para aumentar el recall
- Verificar posibles casos de informaciÃ³n faltante en las respuestas
```

### 4. AnÃ¡lisis de Resultados

#### Archivos Generados
- `evaluate/evaluation_report.txt`: Reporte detallado en formato de texto
- `evaluate/metrics/`: Directorio con mÃ©tricas histÃ³ricas
- `evaluate/failed_cases.json`: Casos que requieren revisiÃ³n manual

#### VisualizaciÃ³n de MÃ©tricas
```bash
# Generar grÃ¡ficos de tendencias
python evaluate.py --graficar
```

### 5. PersonalizaciÃ³n de la EvaluaciÃ³n

Puedes modificar el archivo `evaluate.py` para:
- Ajustar los umbrales de las mÃ©tricas
- Agregar nuevas mÃ©tricas personalizadas
- Cambiar el conjunto de datos de evaluaciÃ³n
- Modificar los prompts de evaluaciÃ³n

### 6. IntegraciÃ³n Continua

El sistema puede integrarse en pipelines CI/CD para monitorear el rendimiento a lo largo del tiempo:

```yaml
# Ejemplo de configuraciÃ³n para GitHub Actions
name: EvaluaciÃ³n RAG

on: [push, pull_request]

jobs:
  evaluate:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    - name: Run evaluation
      run: |
        python evaluate.py --samples 10
    - name: Upload results
      uses: actions/upload-artifact@v3
      with:
        name: evaluation-results
        path: evaluate/
```

## ğŸš¨ SoluciÃ³n de Problemas

### 1. Problemas de Inicio

**SÃ­ntoma**: Error al iniciar la aplicaciÃ³n
```
[ERROR] No se pudo cargar el modelo: ConnectionError
```
**SoluciÃ³n**:
1. Verifica que LM Studio estÃ© ejecutÃ¡ndose y que la URL en `.env` sea correcta
2. Confirma la URL en el archivo `.env`
3. Revisa los logs para mensajes adicionales

### 2. Problemas de Rendimiento

**SÃ­ntoma**: Respuestas lentas o tiempo de espera agotado
```
[WARNING] La generaciÃ³n estÃ¡ tardando mÃ¡s de lo esperado
```
**SoluciÃ³n**:
```bash
# Reducir la carga del sistema
export TOP_K_RETRIEVAL=3
export MAX_TOKENS=512

# Para sistemas con GPU limitada
export CUDA_VISIBLE_DEVICES=0  # Usar solo la primera GPU
```

### 3. OptimizaciÃ³n de GPU para MÃ¡ximo Rendimiento

**SÃ­ntoma**: Bajo uso de GPU o rendimiento por debajo de lo esperado
```
[INFO] Uso de GPU por debajo del 50%
```
**SoluciÃ³n**:
1. Verifica que CUDA estÃ© correctamente instalado:
   ```bash
   nvidia-smi  # Debe mostrar el uso de GPU
   python -c "import torch; print(f'CUDA disponible: {torch.cuda.is_available()}'
   ```
2. Ajusta el tamaÃ±o de lote para mejor uso de GPU:
   ```python
   # En tu configuraciÃ³n
   os.environ['BATCH_SIZE'] = '8'  # Aumentar para mejor uso de GPU
   ```
3. Habilita la precisiÃ³n mixta:
   ```python
   from torch.cuda.amp import autocast
   
   with autocast():
       # Tu cÃ³digo de inferencia aquÃ­
   ```

### 4. Problemas de Memoria

**SÃ­ntoma**: Errores de memoria insuficiente
```
[ERROR] Error al asignar memoria: CUDA out of memory
```
**SoluciÃ³n**:
1. Reduce el tamaÃ±o del lote:
   ```python
   # En tu cÃ³digo Python
   import os
   os.environ['BATCH_SIZE'] = '4'
   ```
2. Usa precisiÃ³n mixta:
   ```python
   model.half()  # Usar precisiÃ³n FP16
   ```
3. Libera memoria de la GPU:
   ```python
   import torch
   torch.cuda.empty_cache()
   ```

### 4. Problemas con los Embeddings

**SÃ­ntoma**: Baja precisiÃ³n en las bÃºsquedas
```
[WARNING] No se encontraron fragmentos relevantes
```
**SoluciÃ³n**:
- Verifica que el modelo de embeddings estÃ© correctamente cargado
- Considera ajustar el umbral de similitud
- Revisa la calidad del texto de entrada

## ğŸš€ PrÃ³ximas Mejoras

### Mejoras en Curso

1. **OptimizaciÃ³n de Rendimiento**
   - [ ] Soporte para cuantizaciÃ³n de modelos
   - [ ] CachÃ© de embeddings en disco
   - [ ] Procesamiento por lotes mejorado

2. **Nuevas Funcionalidades**
   - [ ] Soporte para mÃºltiples formatos de documentos
   - [ ] IntegraciÃ³n con mÃ¡s bases de datos vectoriales
   - [ ] Sistema de plugins para extensiones

3. **Mejoras en la Interfaz**
   - [ ] Interfaz web interactiva
   - [ ] Panel de control de mÃ©tricas
   - [ ] VisualizaciÃ³n de grafos de conocimiento

### CaracterÃ­sticas Futuras

1. **Soporte Multimodal**
   - Procesamiento de imÃ¡genes y tablas
   - BÃºsqueda semÃ¡ntica en mÃºltiples formatos
   - Respuestas enriquecidas con visualizaciones

2. **Aprendizaje AutomÃ¡tico**
   - Mejora continua basada en feedback
   - DetecciÃ³n automÃ¡tica de temas
   - GeneraciÃ³n de resÃºmenes ejecutivos

3. **ColaboraciÃ³n**
   - Compartir fragmentos de documentos
   - Anotaciones colaborativas
   - Sistema de revisiÃ³n por pares

## ğŸ¤ Contribuciones

Â¡Las contribuciones son bienvenidas! Por favor, lee nuestra [guÃ­a de contribuciÃ³n](CONTRIBUTING.md) para mÃ¡s detalles.

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Ver el archivo [LICENSE](LICENSE) para mÃ¡s detalles.

## ğŸ“ Contacto

Para consultas o soporte, por favor contacta a [ron.h.castillo@gmail.com](mailto:ron.h.castillo@gmail.com)

---

<div align="center">
  Hecho con â¤ï¸ por Ronald Castillo Capino
</div>
- **ExtracciÃ³n de texto** con PyPDF2
- **DivisiÃ³n inteligente** que mantiene el contexto
- **Metadatos** para seguimiento de fuentes

### ğŸ”¢ GeneraciÃ³n de Embeddings
- **Modelo**: `sentence-transformers/all-MiniLM-L6-v2`
- **NormalizaciÃ³n**: Vectores unitarios para similitud coseno
- **Rendimiento**: Optimizado para bÃºsqueda rÃ¡pida

### ğŸ” RecuperaciÃ³n
- **Motor**: FAISS para bÃºsqueda vectorial
- **Estrategia**: Top-5 fragmentos mÃ¡s relevantes
- **Rendimiento**: Respuestas en tiempo real

### ğŸ¤– GeneraciÃ³n
- **Modelo Local**: `llama-3.1-nemotron-nano-8b-v1`
- **TÃ©cnicas Avanzadas**:
  - Prompt engineering
  - Control de contexto
  - VerificaciÃ³n de hechos

## ğŸ“ˆ Resultados Esperados

El sistema estÃ¡ diseÃ±ado para superar los siguientes umbrales:

| MÃ©trica | Objetivo | ExplicaciÃ³n |
|---------|----------|-------------|
| ğŸ¯ Fidelidad | > 0.90 | Las respuestas se basan estrictamente en el contexto |
| ğŸ¯ Relevancia | > 0.85 | Las respuestas responden directamente a la pregunta |
| ğŸ¯ PrecisiÃ³n | > 0.80 | Los fragmentos recuperados son relevantes |
| ğŸ¯ Recall | > 0.75 | Se recupera la mayorÃ­a de la informaciÃ³n relevante |

ğŸ’¡ Estos valores pueden variar segÃºn la calidad del documento fuente y la complejidad de las preguntas.

## ğŸš¨ SoluciÃ³n de Problemas

### Problemas Comunes y Soluciones:

1. **ğŸ”‘ Error de AutenticaciÃ³n**
   ```
   [ERROR] Error de autenticaciÃ³n con el modelo
   ```
   ğŸ”§ **SoluciÃ³n**: Verifica que LM Studio estÃ© ejecutÃ¡ndose y que la URL en `.env` sea correcta

2. **ğŸ“„ Archivo PDF no encontrado**
   ```
   [ERROR] No se encontrÃ³ el archivo PDF
   ```
   ğŸ”§ **SoluciÃ³n**: AsegÃºrate de que el archivo existe en `data/PDF-GenAI-Challenge.pdf`

3. **ğŸŒ Rendimiento lento**
   ```
   [INFO] La generaciÃ³n de respuestas estÃ¡ tardando mÃ¡s de lo esperado
   ```
   ğŸ”§ **SoluciÃ³n**:
   - Reduce el nÃºmero de fragmentos recuperados
   - Usa un modelo mÃ¡s pequeÃ±o
   - Verifica el rendimiento de tu hardware

4. **ğŸ’¾ Problemas de memoria**
   ```
   MemoryError: No se puede asignar memoria
   ```
   ğŸ”§ **SoluciÃ³n**:
   - Reduce el tamaÃ±o del lote de procesamiento
   - Cierra otras aplicaciones que consuman mucha memoria
   - Considera usar un equipo con mÃ¡s RAM

## ğŸš€ PrÃ³ximos Pasos

### Mejoras Planificadas:

1. **ğŸ¯ Mejora de PrecisiÃ³n**
   - Implementar re-ranking de resultados
   - AÃ±adir verificaciÃ³n cruzada de hechos
   - Mejorar la recuperaciÃ³n de contexto

2. **âš¡ Rendimiento**
   - Optimizar el uso de memoria
   - Implementar cachÃ© de embeddings
   - Soporte para procesamiento por lotes

3. **ğŸŒ Interfaz de Usuario**
   - Desarrollar interfaz web interactiva
   - AÃ±adir visualizaciÃ³n de fuentes
   - Soporte para mÃºltiples formatos de documentos

4. **ğŸ“ˆ Escalabilidad**
   - Soporte para mÃºltiples documentos
   - BÃºsqueda distribuida
   - IndexaciÃ³n incremental

## ğŸ“ Soporte

### Â¿Necesitas ayuda?

1. **ğŸ“‹ Verifica los logs** en la consola para mensajes de error detallados
2. **ğŸ“Š Revisa el reporte** en `evaluate/evaluation_report.txt`
3. **ğŸ” Comprueba** la configuraciÃ³n en `.env`
4. **ğŸ“š Consulta la documentaciÃ³n** de las dependencias

### Â¿Sigues teniendo problemas?

- ğŸ“§ EnvÃ­a un correo a [ron.h.castillo@gmail.com](mailto:ron.h.castillo@gmail.com)
- ğŸ”— Incluye los mensajes de error y los pasos para reproducir el problema

---

âœ¨ **Desarrollado con â¤ï¸ para el DesafÃ­o de Ingeniero de IA** âœ¨

---

### ğŸ“š Recursos Adicionales

- [DocumentaciÃ³n de RAGAS](https://github.com/explodinggradients/ragas)
- [GuÃ­a de FAISS](https://github.com/facebookresearch/faiss)
- [DocumentaciÃ³n de LM Studio](https://lmstudio.ai/docs/)

ğŸ¯ **Objetivo del Proyecto**: Crear un sistema de preguntas y respuestas confiable y escalable para documentaciÃ³n tÃ©cnica.