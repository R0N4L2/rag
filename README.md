# ğŸ“š Sistema RAG para Control de Calidad del Conocimiento Interno

<div align="center">
  <img src="https://img.shields.io/badge/Python-3.8%2B-blue" alt="Python 3.8+">
  <img src="https://img.shields.io/badge/Framework-LangChain-yellow" alt="LangChain">
  <img src="https://img.shields.io/badge/Vector%20Store-FAISS-ff69b4" alt="FAISS">
  <img src="https://img.shields.io/badge/LLM-llama--3.1--nemotron--nano--8b--v1-orange" alt="LLM Model">
</div>

ğŸ‘¨â€ğŸ’» **Autor**: Ronald Castillo Capino  
ğŸ“§ **Contacto**: [ron.h.castillo@gmail.com](mailto:ron.h.castillo@gmail.com)

> ğŸ’¡ Este proyecto implementa un sistema avanzado de Preguntas y Respuestas (Q&A) que combina recuperaciÃ³n de informaciÃ³n con generaciÃ³n de lenguaje natural, garantizando respuestas precisas, verificables y basadas en documentos especÃ­ficos.

## ğŸš€ DescripciÃ³n del Proyecto

Este sistema **RAG (Retrieval-Augmented Generation)** estÃ¡ diseÃ±ado para proporcionar respuestas precisas y contextualizadas mediante la combinaciÃ³n de:

- ğŸ” **RecuperaciÃ³n de informaciÃ³n** avanzada de documentos tÃ©cnicos
- ğŸ§  **GeneraciÃ³n de respuestas** utilizando el modelo local `Llama-3.1-Nemotron-Nano-4B-v1.1` con aceleraciÃ³n por GPU
- ğŸ“Š **EvaluaciÃ³n automÃ¡tica** de la calidad con mÃ©tricas RAGAS
- ğŸš€ **AceleraciÃ³n por GPU** para un rendimiento Ã³ptimo en inferencia local

ğŸ’¡ **Caso de Uso Principal**: Sistema experto de preguntas y respuestas sobre el libro "An Introduction to Statistical Learning with Applications in Python", permitiendo a los usuarios obtener explicaciones claras y precisas sobre conceptos de aprendizaje estadÃ­stico.

### ğŸš€ DesempeÃ±o con AceleraciÃ³n por GPU

El sistema estÃ¡ optimizado para aprovechar al mÃ¡ximo la capacidad de procesamiento en paralelo de tu GPU local, ofreciendo tiempos de respuesta rÃ¡pidos con el modelo Llama-3.1-Nemotron-Nano-8B. La configuraciÃ³n incluye:

- **Contexto extendido**: 16,384 tokens
- **Capas GPU**: 30 capas (ajustable segÃºn VRAM)
- **Procesamiento por lotes**: 512 tokens
- **Hilos de CPU**: 8 hilos

### ğŸ”§ Requisitos de Hardware

| Componente | MÃ­nimo | Recomendado |
|------------|--------|-------------|
| **GPU** | NVIDIA RTX 3060 | NVIDIA RTX 4090 o superior |
| **VRAM** | 12GB | 24GB+ |
| **RAM del Sistema** | 32GB | 64GB+ |
| **Almacenamiento** | 15GB libres | SSD NVMe |

### âš™ï¸ ConfiguraciÃ³n Ã“ptima

1. **Controladores NVIDIA**
   ```bash
   # Verificar instalaciÃ³n de controladores
   nvidia-smi
   # VersiÃ³n mÃ­nima recomendada: 525.60.13
   ```

2. **Bibliotecas CUDA**
   - CUDA Toolkit 11.7 o superior
   - cuDNN 8.5 o superior
   - Verificar instalaciÃ³n:
     ```bash
     nvcc --version
     ```

### ğŸš€ Optimizaciones Implementadas

- **Inferencia Acelerada por GPU**
  - Todas las operaciones del modelo se ejecutan en la GPU
  - Soporte para CUDA y cuBLAS para operaciones matriciales

- **GestiÃ³n Eficiente de Memoria**
  - Carga selectiva de capas del modelo
  - OptimizaciÃ³n de memoria intermedia
  - Soporte para precisiÃ³n mixta (FP16/FP32)

- **Procesamiento por Lotes**
  - Procesamiento paralelo de mÃºltiples consultas
  - Ajuste automÃ¡tico del tamaÃ±o de lote segÃºn la VRAM disponible

### ğŸ“Š Rendimiento Esperado

| ConfiguraciÃ³n | Tokens/seg | Memoria GPU |
|---------------|------------|-------------|
| RTX 3060 (12GB) | 18-25 | ~12GB |
| RTX 3090 (24GB) | 30-40 | ~22GB |
| RTX 4090 (24GB) | 45-60 | ~24GB |

*Nota: El rendimiento puede variar segÃºn la carga del sistema y la configuraciÃ³n especÃ­fica.*

### CaracterÃ­sticas Clave

- âœ… **Respuestas Basadas en Contexto**: Cada respuesta estÃ¡ respaldada por fragmentos especÃ­ficos del documento
- ğŸ” **BÃºsqueda SemÃ¡ntica**: Encuentra informaciÃ³n relevante incluso con consultas en lenguaje natural
- ğŸ“ˆ **EvaluaciÃ³n Continua**: Sistema integrado para medir y mejorar la calidad de las respuestas
- ğŸš€ **Rendimiento Optimizado**: DiseÃ±ado para funcionar eficientemente en hardware estÃ¡ndar

## ğŸ¯ Objetivo

Desarrollar un asistente de IA que:

âœ… Proporcione respuestas precisas basadas en documentos especÃ­ficos  
âœ… Mantenga la trazabilidad de las fuentes de informaciÃ³n  
âœ… EvalÃºe automÃ¡ticamente la calidad de las respuestas  
âœ… Sea fÃ¡cil de implementar y mantener

## ğŸ—ï¸ Arquitectura del Sistema

```mermaid
graph TD
    A[Usuario] -->|Pregunta| B[Procesamiento de Texto]
    B --> C[GeneraciÃ³n de Embeddings]
    C --> D[Base de Vectores FAISS]
    D -->|Contexto Relevante| E[Modelo de Lenguaje]
    E -->|Respuesta| A
    F[Documentos] -->|Procesamiento| G[Base de Conocimiento]
    G --> D
    H[MÃ³dulo de EvaluaciÃ³n] <-->|MÃ©tricas| E
```

### ğŸ”§ Componentes Principales

#### 1. **Procesamiento de Documentos**
- **ExtracciÃ³n de Texto**: Utiliza PyPDF2 para extraer texto de documentos PDF
- **Limpieza de Texto**: EliminaciÃ³n de caracteres especiales, normalizaciÃ³n de espacios
- **TokenizaciÃ³n**: DivisiÃ³n del texto en unidades significativas
- **FragmentaciÃ³n SemÃ¡ntica**:
  - TamaÃ±o de fragmento: 4000 caracteres
  - Solapamiento: 200 caracteres
  - PreservaciÃ³n de contexto entre fragmentos

#### 2. **Modelo de Embeddings**
- **Modelo**: `sentence-transformers/all-MiniLM-L6-v2`
- **DimensiÃ³n de Embeddings**: 384
- **NormalizaciÃ³n**: Vectores unitarios para similitud coseno
- **Rendimiento**: Optimizado para equilibrio entre precisiÃ³n y velocidad

#### 3. **Almacenamiento Vectorial (FAISS)**
- **Ãndice**: `IndexFlatL2` para bÃºsqueda exacta
- **MÃ©tricas**: Distancia euclidiana (L2)
- **Optimizaciones**:
  - BÃºsqueda por lotes
  - Filtrado por umbral de similitud
  - RecuperaciÃ³n de los 5 mejores resultados

#### 4. **Modelo de Lenguaje Local**
- **Modelo Base**: `Llama-3.1-Nemotron-Nano-8B-v1`
- **EjecuciÃ³n**:
  - Ejecutado localmente a travÃ©s de LM Studio
  - AceleraciÃ³n por GPU para mÃ¡ximo rendimiento
  - VersiÃ³n cuantizada (Q4_K_S) para eficiencia
- **ConfiguraciÃ³n**:
  - Temperatura: 0.1 (para respuestas deterministas)
  - Tokens mÃ¡ximos: 2048
  - Contexto: 16,384 tokens
  - Capas GPU: 30 (ajustable segÃºn VRAM)
  - Procesamiento por lotes: 512 tokens
  - Hilos CPU: 8
- **Prompt Engineering**:
  - Instrucciones claras para el modelo
  - Formato estructurado de respuestas
  - Manejo de incertidumbre

#### 5. **MÃ³dulo de EvaluaciÃ³n**
- **MÃ©tricas RAGAS**:
  - Faithfulness (Fidelidad)
  - Answer Relevancy (Relevancia de la respuesta)
  - Context Precision (PrecisiÃ³n del contexto)
  - Context Recall (RecuperaciÃ³n del contexto)
  - Answer Similarity (Similitud de respuestas)
  - Answer Correctness (CorrecciÃ³n de respuestas)
  - Harmfulness (Contenido daÃ±ino)
- **Reportes**:
  - Salida detallada en consola
  - Archivo de registro estructurado
  - EstadÃ­sticas agregadas

#### 6. **API y Servicios**
- **Interfaz de LÃ­nea de Comandos (CLI)**
- **API RESTful** (opcional)
- **IntegraciÃ³n con LM Studio**
  - Endpoint: `http://localhost:1234/v1`
  - Soporte para streaming
  - Manejo de tiempo de espera

### ğŸ”„ Flujo de Datos

1. **Ingreso de Consulta**
   - El usuario ingresa una pregunta en lenguaje natural
   - La consulta se normaliza y procesa

2. **BÃºsqueda SemÃ¡ntica**
   - La consulta se convierte en embedding
   - Se buscan los fragmentos mÃ¡s similares en FAISS
   - Se recuperan los 5 fragmentos mÃ¡s relevantes

3. **GeneraciÃ³n de Respuesta**
   - Los fragmentos recuperados se combinan con el prompt
   - El modelo de lenguaje genera una respuesta contextualizada
   - Se aplican filtros de seguridad y calidad

4. **RetroalimentaciÃ³n y Mejora**
   - La interacciÃ³n se registra para evaluaciÃ³n
   - Las mÃ©tricas se calculan y almacenan
   - El sistema se ajusta segÃºn el rendimiento

## âš™ï¸ ConfiguraciÃ³n del Entorno

El archivo `.env` contiene las siguientes configuraciones clave:

```ini
# ConfiguraciÃ³n del Modelo
MODEL_NAME=llama-3.1-nemotron-nano-8b-v1
LOCAL_MODEL_PATH=ruta/al/modelo.gguf

# ConfiguraciÃ³n de Rendimiento
N_CTX=16384
N_GPU_LAYERS=30
N_BATCH=512
N_THREADS=8

# ConfiguraciÃ³n de GeneraciÃ³n
LLM_TEMPERATURE=0.1
MAX_TOKENS=2048

# ConfiguraciÃ³n de FragmentaciÃ³n
CHUNK_SIZE=4000
CHUNK_OVERLAP=200
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# ConfiguraciÃ³n de RecuperaciÃ³n
TOP_K_RETRIEVAL=5
CONTEXT_WINDOW=131072
```

## ğŸ“ Estructura del Proyecto

```
.
â”œâ”€â”€ data/                           # Directorio de datos
â”‚   â””â”€â”€ PDF-GenAI-Challenge.pdf    # Documento fuente para el sistema RAG
â”‚
â”œâ”€â”€ evaluate/                      # Resultados de evaluaciÃ³n
â”‚   â””â”€â”€ evaluation_report.txt      # Reporte detallado de mÃ©tricas
â”‚
â”œâ”€â”€ .env                   # Plantilla de configuraciÃ³n
â”œâ”€â”€ main.py                       # Punto de entrada principal
â”œâ”€â”€ evaluate.py                   # MÃ³dulo de evaluaciÃ³n
â”œâ”€â”€ utils.py                      # Utilidades y helpers
â”œâ”€â”€ requirements.txt              # Dependencias del proyecto
â””â”€â”€ README.md                     # DocumentaciÃ³n
```

### ğŸ“‹ Archivos Principales

#### `main.py`
Punto de entrada principal que implementa:
- Carga y procesamiento de documentos
- Interfaz de lÃ­nea de comandos
- IntegraciÃ³n de componentes RAG
- Manejo de errores y logging

#### `evaluate.py`
MÃ³dulo de evaluaciÃ³n que incluye:
- ImplementaciÃ³n de mÃ©tricas RAGAS
- GeneraciÃ³n de reportes detallados
- Herramientas para anÃ¡lisis comparativo
- VisualizaciÃ³n de resultados

#### `utils.py`
Funciones auxiliares para:
- Procesamiento de texto y PDF
- Manejo de embeddings
- Utilidades de sistema
- ConfiguraciÃ³n y logging

#### `requirements.txt`
Lista de dependencias con versiones especÃ­ficas para garantizar compatibilidad.

#### `.env`
Archivo de configuraciÃ³n que debe contener:
- Rutas a modelos
- Configuraciones de ejecuciÃ³n
- ParÃ¡metros del sistema

## âš™ï¸ InstalaciÃ³n y ConfiguraciÃ³n

### 1. ğŸ› ï¸ Requisitos Previos

- Python 3.8 o superior
- pip (gestor de paquetes de Python)
- Git (opcional, para clonar el repositorio)
- LM Studio (para ejecutar modelos locales)
- Al menos 8GB de RAM (16GB recomendado)

### 2. ğŸ—ï¸ ConfiguraciÃ³n del Entorno

```bash
# 1. Clonar el repositorio (opcional)
git clone <repo-url>
cd rag_challenge

# 2. Crear y activar entorno virtual
python -m venv rag_env
source rag_env/bin/activate  # Linux/Mac
# o
rag_env\Scripts\activate     # Windows

# 3. Actualizar pip
python -m pip install --upgrade pip

# 4. Instalar dependencias
pip install -r requirements.txt
```

### 3. ğŸ” ConfiguraciÃ³n de Variables de Entorno

1. Copiar el archivo de ejemplo:
   ```bash
   cp .env.example .env
   ```

2. Editar el archivo `.env` con tus configuraciones:
   ```env
   # ===== ConfiguraciÃ³n del Modelo =====
   LOCAL_LLM_URL=http://localhost:1234/v1
   MODEL_NAME=llama-3.1-nemotron-nano-8b-v1
   
   # ===== ConfiguraciÃ³n de Embeddings =====
   EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
   
   # ===== ConfiguraciÃ³n de Procesamiento =====
   CHUNK_SIZE=4000
   CHUNK_OVERLAP=200
   TOP_K_RETRIEVAL=5
   
   # ===== ConfiguraciÃ³n del Modelo =====
   LLM_TEMPERATURE=0.1
   MAX_TOKENS=1024
   CONTEXT_WINDOW=2048
   
   # ===== ConfiguraciÃ³n de EvaluaciÃ³n =====
   EVAL_SAMPLES=5
   EVALUATION_OUTPUT_PATH=evaluate/evaluation_report.txt
   
   # ===== ConfiguraciÃ³n del Sistema =====
   LOG_LEVEL=INFO
   CACHE_DIR=./cache
   ```

### 4. ğŸ“¦ Dependencias Principales

El archivo `requirements.txt` contiene todas las dependencias necesarias:

```
# Procesamiento de documentos
PyPDF2>=3.0.0
python-dotenv>=1.0.0

# Procesamiento de lenguaje natural
sentence-transformers>=2.2.2
numpy>=1.24.0

# Almacenamiento vectorial
faiss-cpu>=1.7.4  # o faiss-gpu si tienes CUDA

# Modelo de lenguaje
llama-cpp-python>=0.2.0

# EvaluaciÃ³n
ragas>=0.0.21
datasets>=2.14.0

# Utilidades
tqdm>=4.65.0
colorama>=0.4.6
python-dotenv>=1.0.0
```

### 5. ğŸš€ ConfiguraciÃ³n del Modelo Local

#### Descarga e InstalaciÃ³n del Modelo

1. **Descargar el modelo**:
   - Nombre: `Llama-3.1-Nemotron-Nano-4B-v1.1-GGUF`
   - TamaÃ±o: ~2.5GB (versiÃ³n cuantizada Q4_K_M)
   - UbicaciÃ³n por defecto: `C:\Users\[usuario]\.cache\lm-studio\models\`

2. **ConfiguraciÃ³n en LM Studio**:
   - Abrir LM Studio y seleccionar "Download a model"
   - Buscar: `llama-3.1-nemotron-nano-8b-v1-GGUF`
   - Descargar la versiÃ³n `Q4_K_M` para el mejor equilibrio entre rendimiento y calidad

3. **Habilitar AceleraciÃ³n por GPU**:
   - Ir a ConfiguraciÃ³n â†’ Modelo
   - Seleccionar "Auto" o tu GPU especÃ­fica en "GPU Layers"
   - Activar "Use CUDA"
   - Establecer "Context Length" a 2048 tokens

4. **Iniciar el Servidor Local**:
   - Ir a la pestaÃ±a "Local Server"
   - Asegurarse de que "GPU Offload" estÃ© activado
   - Hacer clic en "Start Server"
   - Verificar que la URL sea `http://localhost:1234/v1`

5. **Verificar la ConfiguraciÃ³n de GPU**:
   - Abrir el Administrador de Tareas de Windows
   - Ir a la pestaÃ±a "Rendimiento"
   - Verificar que la GPU muestre actividad durante la inferencia
   - Confirmar que la memoria de GPU se estÃ© utilizando

#### ConfiguraciÃ³n Recomendada para Ã“ptimo Rendimiento

```env
# En tu archivo .env
LOCAL_LLM_URL=http://localhost:1234/v1
MODEL_NAME=llama-3.1-nemotron-nano-8b-v1
LLM_TEMPERATURE=0.1
MAX_TOKENS=1024
CONTEXT_WINDOW=2048
```

2. **Preparar los datos**:
   ```bash
   mkdir -p data
   cp /ruta/a/tu/documento.pdf data/PDF-GenAI-Challenge.pdf
   ```

3. **Verificar la instalaciÃ³n**:
   ```bash
   python -c "import torch; print(f'PyTorch: {torch.__version__}')"
   python -c "import transformers; print(f'Transformers: {transformers.__version__}')"
   ```

### 6. ğŸ§ª Prueba RÃ¡pida

```bash
# Ejecutar una consulta de prueba
echo "Â¿QuÃ© es el aprendizaje automÃ¡tico?" | python main.py

# Ejecutar evaluaciÃ³n bÃ¡sica
python evaluate.py --samples 3
```

### ğŸ“ Notas de InstalaciÃ³n

- Para mejor rendimiento, se recomienda usar una GPU compatible con CUDA
- El primer inicio puede tardar varios minutos mientras se descargan los modelos
- Verifica que el puerto 1234 estÃ© disponible para LM Studio

## ğŸ’» Uso del Sistema

### 1. Modos de OperaciÃ³n

#### Modo Interactivo
```bash
# Iniciar el sistema en modo interactivo
python main.py

# Ejemplo de sesiÃ³n:
Bienvenido al Sistema RAG de Control de Calidad
> Â¿Sobre quÃ© tema te gustarÃ­a consultar?
```

#### Modo por LÃ­nea de Comandos
```bash
# Hacer una pregunta especÃ­fica
python main.py --pregunta "Â¿QuÃ© es el trade-off entre sesgo y varianza?"

# Procesar un archivo con mÃºltiples preguntas
python main.py --archivo preguntas.txt --salida respuestas.json

# Opciones adicionales
python main.py \
  --pregunta "Explique la regresiÃ³n lineal" \
  --temperatura 0.3 \
  --max-tokens 500 \
  --mostrar-fuentes
```

### 2. Ejemplos de Preguntas

#### Conceptos BÃ¡sicos
```
- "Â¿QuÃ© es el aprendizaje supervisado?"
- "Explique el concepto de validaciÃ³n cruzada"
- "Â¿CuÃ¡les son las ventajas de los Ã¡rboles de decisiÃ³n?"
```

#### Comparaciones
```
- "Compare ridge y lasso regression"
- "Diferencia entre bagging y boosting"
- "Ventajas de SVM sobre regresiÃ³n logÃ­stica"
```

#### Aplicaciones PrÃ¡cticas
```
- "Â¿CÃ³mo manejar datos faltantes en un dataset?"
- "TÃ©cnicas para tratar el desbalance de clases"
- "MÃ©todos de selecciÃ³n de caracterÃ­sticas"
```

### 3. Opciones Avanzadas

#### ConfiguraciÃ³n de BÃºsqueda
```bash
# Ajustar el nÃºmero de fragmentos recuperados
python main.py --top-k 3

# Cambiar el umbral de similitud (0-1)
python main.py --umbral-similitud 0.75
```

#### Control de Salida
```bash
# Mostrar solo la respuesta sin fuentes
python main.py --pregunta "..." --formato simple

# Generar salida en formato JSON
python main.py --pregunta "..." --formato json

# Guardar resultados en un archivo
python main.py --pregunta "..." --salida resultado.txt
```

## ğŸ“Š EvaluaciÃ³n del Sistema

### 1. MÃ©tricas Implementadas

El sistema utiliza el framework RAGAS para evaluar la calidad de las respuestas con las siguientes mÃ©tricas:

| MÃ©trica | Rango Ã“ptimo | DescripciÃ³n |
|---------|--------------|-------------|
| **Faithfulness** | 0.8 - 1.0 | Mide si la respuesta se basa Ãºnicamente en el contexto |
| **Answer Relevancy** | > 0.7 | EvalÃºa la relevancia de la respuesta |
| **Context Precision** | > 0.6 | PrecisiÃ³n de los fragmentos recuperados |
| **Context Recall** | > 0.7 | Capacidad de recuperar informaciÃ³n relevante |
| **Answer Similarity** | > 0.75 | ComparaciÃ³n con respuestas de referencia |
| **Answer Correctness** | > 0.8 | PrecisiÃ³n fÃ¡ctica de la respuesta |
| **Harmfulness** | < 0.2 | DetecciÃ³n de contenido potencialmente daÃ±ino |

### 2. EjecuciÃ³n de la EvaluaciÃ³n

#### EvaluaciÃ³n BÃ¡sica
```bash
# Evaluar con 5 ejemplos (valor por defecto)
python evaluate.py
```

#### EvaluaciÃ³n Personalizada
```bash
# Especificar nÃºmero de muestras
python evaluate.py --muestras 10

# Evaluar mÃ©tricas especÃ­ficas
python evaluate.py --metricas fidelidad relevancia

# Generar reporte en formato JSON
python evaluate.py --formato json --salida resultados.json
```

#### EvaluaciÃ³n con Conjunto de Datos Personalizado
```bash
# Usar un archivo JSON con preguntas y respuestas de referencia
python evaluate.py --dataset datos_evaluacion.json
```

### 3. InterpretaciÃ³n de Resultados

#### Ejemplo de Salida
```
========================================
   RESULTADOS DE LA EVALUACIÃ“N RAGAS   
========================================

- Faithfulness: 0.87
  âœ“ Excelente: La respuesta se basa completamente en el contexto

- Answer Relevancy: 0.82
  âœ“ Muy buena: La respuesta es altamente relevante a la pregunta

- Context Precision: 0.75
  âœ“ Buena: La mayorÃ­a de los fragmentos recuperados son relevantes

- Context Recall: 0.68
  âœ“ Aceptable: Se recupera la mayor parte de la informaciÃ³n relevante

- Answer Similarity: 0.79
  âœ“ Buena: La respuesta es similar a la referencia esperada

- Answer Correctness: 0.83
  âœ“ Muy buena: La informaciÃ³n proporcionada es correcta

- Harmfulness: 0.05
  âœ“ Seguro: No se detectÃ³ contenido daÃ±ino

----------------------------------------
PuntuaciÃ³n Promedio: 0.79
Estado General: Buen rendimiento
----------------------------------------

ğŸ“ Recomendaciones:
- Mejorar la recuperaciÃ³n de contexto para aumentar el recall
- Verificar posibles casos de informaciÃ³n faltante en las respuestas
```

### 4. AnÃ¡lisis de Resultados

#### Archivos Generados
- `evaluate/evaluation_report.txt`: Reporte detallado en formato de texto
- `evaluate/metrics/`: Directorio con mÃ©tricas histÃ³ricas
- `evaluate/failed_cases.json`: Casos que requieren revisiÃ³n manual

#### VisualizaciÃ³n de MÃ©tricas
```bash
# Generar grÃ¡ficos de tendencias
python evaluate.py --graficar
```

### 5. PersonalizaciÃ³n de la EvaluaciÃ³n

Puedes modificar el archivo `evaluate.py` para:
- Ajustar los umbrales de las mÃ©tricas
- Agregar nuevas mÃ©tricas personalizadas
- Cambiar el conjunto de datos de evaluaciÃ³n
- Modificar los prompts de evaluaciÃ³n

### 6. IntegraciÃ³n Continua

El sistema puede integrarse en pipelines CI/CD para monitorear el rendimiento a lo largo del tiempo:

```yaml
# Ejemplo de configuraciÃ³n para GitHub Actions
name: EvaluaciÃ³n RAG

on: [push, pull_request]

jobs:
  evaluate:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    - name: Run evaluation
      run: |
        python evaluate.py --samples 10
    - name: Upload results
      uses: actions/upload-artifact@v3
      with:
        name: evaluation-results
        path: evaluate/
```

## ğŸš¨ SoluciÃ³n de Problemas Comunes

### 1. Problemas de Inicio

**SÃ­ntoma**: Error al iniciar la aplicaciÃ³n
```
[ERROR] No se pudo cargar el modelo: ConnectionError
```
**SoluciÃ³n**:
1. Verifica que el servidor de LM Studio estÃ© en ejecuciÃ³n
2. Confirma la URL en el archivo `.env`
3. Revisa los logs para mensajes adicionales

### 2. Problemas de Rendimiento

**SÃ­ntoma**: Respuestas lentas o tiempo de espera agotado
```
[WARNING] La generaciÃ³n estÃ¡ tardando mÃ¡s de lo esperado
```
**SoluciÃ³n**:
```bash
# Reducir la carga del sistema
export TOP_K_RETRIEVAL=3
export MAX_TOKENS=512

# Para sistemas con GPU limitada
export CUDA_VISIBLE_DEVICES=0  # Usar solo la primera GPU
```

### 3. OptimizaciÃ³n de GPU para MÃ¡ximo Rendimiento

**SÃ­ntoma**: Bajo uso de GPU o rendimiento por debajo de lo esperado
```
[INFO] Uso de GPU por debajo del 50%
```
**SoluciÃ³n**:
1. Verifica que CUDA estÃ© correctamente instalado:
   ```bash
   nvidia-smi  # Debe mostrar el uso de GPU
   python -c "import torch; print(f'CUDA disponible: {torch.cuda.is_available()}')
   ```
2. Ajusta el tamaÃ±o de lote para mejor uso de GPU:
   ```python
   # En tu configuraciÃ³n
   os.environ['BATCH_SIZE'] = '8'  # Aumentar para mejor uso de GPU
   ```
3. Habilita la precisiÃ³n mixta:
   ```python
   from torch.cuda.amp import autocast
   
   with autocast():
       # Tu cÃ³digo de inferencia aquÃ­
   ```

### 4. Problemas de Memoria

**SÃ­ntoma**: Errores de memoria insuficiente
```
[ERROR] Error al asignar memoria: CUDA out of memory
```
**SoluciÃ³n**:
1. Reduce el tamaÃ±o del lote:
   ```python
   # En tu cÃ³digo Python
   import os
   os.environ['BATCH_SIZE'] = '4'
   ```
2. Usa precisiÃ³n mixta:
   ```python
   model.half()  # Usar precisiÃ³n FP16
   ```
3. Libera memoria de la GPU:
   ```python
   import torch
   torch.cuda.empty_cache()
   ```

### 4. Problemas con los Embeddings

**SÃ­ntoma**: Baja precisiÃ³n en las bÃºsquedas
```
[WARNING] No se encontraron fragmentos relevantes
```
**SoluciÃ³n**:
- Verifica que el modelo de embeddings estÃ© correctamente cargado
- Considera ajustar el umbral de similitud
- Revisa la calidad del texto de entrada

## ğŸš€ PrÃ³ximas Mejoras

### Mejoras en Curso

1. **OptimizaciÃ³n de Rendimiento**
   - [ ] Soporte para cuantizaciÃ³n de modelos
   - [ ] CachÃ© de embeddings en disco
   - [ ] Procesamiento por lotes mejorado

2. **Nuevas Funcionalidades**
   - [ ] Soporte para mÃºltiples formatos de documentos
   - [ ] IntegraciÃ³n con mÃ¡s bases de datos vectoriales
   - [ ] Sistema de plugins para extensiones

3. **Mejoras en la Interfaz**
   - [ ] Interfaz web interactiva
   - [ ] Panel de control de mÃ©tricas
   - [ ] VisualizaciÃ³n de grafos de conocimiento

### CaracterÃ­sticas Futuras

1. **Soporte Multimodal**
   - Procesamiento de imÃ¡genes y tablas
   - BÃºsqueda semÃ¡ntica en mÃºltiples formatos
   - Respuestas enriquecidas con visualizaciones

2. **Aprendizaje AutomÃ¡tico**
   - Mejora continua basada en feedback
   - DetecciÃ³n automÃ¡tica de temas
   - GeneraciÃ³n de resÃºmenes ejecutivos

3. **ColaboraciÃ³n**
   - Compartir fragmentos de documentos
   - Anotaciones colaborativas
   - Sistema de revisiÃ³n por pares
## ğŸ¤ Contribuciones

Â¡Las contribuciones son bienvenidas! Por favor, lee nuestra [guÃ­a de contribuciÃ³n](CONTRIBUTING.md) para mÃ¡s detalles.

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Ver el archivo [LICENSE](LICENSE) para mÃ¡s detalles.

## ğŸ“ Contacto

Para consultas o soporte, por favor contacta a [ron.h.castillo@gmail.com](mailto:ron.h.castillo@gmail.com)

---

<div align="center">
  Hecho con â¤ï¸ por Ronald Castillo Capino
</div>
- **ExtracciÃ³n de texto** con PyPDF2
- **DivisiÃ³n inteligente** que mantiene el contexto
- **Metadatos** para seguimiento de fuentes

### ğŸ”¢ GeneraciÃ³n de Embeddings
- **Modelo**: `sentence-transformers/all-MiniLM-L6-v2`
- **NormalizaciÃ³n**: Vectores unitarios para similitud coseno
- **Rendimiento**: Optimizado para bÃºsqueda rÃ¡pida

### ğŸ” RecuperaciÃ³n
- **Motor**: FAISS para bÃºsqueda vectorial
- **Estrategia**: Top-5 fragmentos mÃ¡s relevantes
- **Rendimiento**: Respuestas en tiempo real

### ğŸ¤– GeneraciÃ³n
- **Modelo Local**: `llama-3.1-nemotron-nano-8b-v1`
- **TÃ©cnicas Avanzadas**:
  - Prompt engineering
  - Control de contexto
  - VerificaciÃ³n de hechos

## ğŸ“ˆ Resultados Esperados

El sistema estÃ¡ diseÃ±ado para superar los siguientes umbrales:

| MÃ©trica | Objetivo | ExplicaciÃ³n |
|---------|----------|-------------|
| ğŸ¯ Fidelidad | > 0.90 | Las respuestas se basan estrictamente en el contexto |
| ğŸ¯ Relevancia | > 0.85 | Las respuestas responden directamente a la pregunta |
| ğŸ¯ PrecisiÃ³n | > 0.80 | Los fragmentos recuperados son relevantes |
| ğŸ¯ Recall | > 0.75 | Se recupera la mayorÃ­a de la informaciÃ³n relevante |

ğŸ’¡ Estos valores pueden variar segÃºn la calidad del documento fuente y la complejidad de las preguntas.

## ğŸš¨ SoluciÃ³n de Problemas

### Problemas Comunes y Soluciones:

1. **ğŸ”‘ Error de AutenticaciÃ³n**
   ```
   [ERROR] Error de autenticaciÃ³n con el modelo
   ```
   ğŸ”§ **SoluciÃ³n**: Verifica que LM Studio estÃ© ejecutÃ¡ndose y que la URL en `.env` sea correcta

2. **ğŸ“„ Archivo PDF no encontrado**
   ```
   [ERROR] No se encontrÃ³ el archivo PDF
   ```
   ğŸ”§ **SoluciÃ³n**: AsegÃºrate de que el archivo existe en `data/PDF-GenAI-Challenge.pdf`

3. **ğŸŒ Rendimiento lento**
   ```
   [INFO] La generaciÃ³n de respuestas estÃ¡ tardando mÃ¡s de lo esperado
   ```
   ğŸ”§ **SoluciÃ³n**:
   - Reduce el nÃºmero de fragmentos recuperados
   - Usa un modelo mÃ¡s pequeÃ±o
   - Verifica el rendimiento de tu hardware

4. **ğŸ’¾ Problemas de memoria**
   ```
   MemoryError: No se puede asignar memoria
   ```
   ğŸ”§ **SoluciÃ³n**:
   - Reduce el tamaÃ±o del lote de procesamiento
   - Cierra otras aplicaciones que consuman mucha memoria
   - Considera usar un equipo con mÃ¡s RAM

## ğŸš€ PrÃ³ximos Pasos

### Mejoras Planificadas:

1. **ğŸ¯ Mejora de PrecisiÃ³n**
   - Implementar re-ranking de resultados
   - AÃ±adir verificaciÃ³n cruzada de hechos
   - Mejorar la recuperaciÃ³n de contexto

2. **âš¡ Rendimiento**
   - Optimizar el uso de memoria
   - Implementar cachÃ© de embeddings
   - Soporte para procesamiento por lotes

3. **ğŸŒ Interfaz de Usuario**
   - Desarrollar interfaz web interactiva
   - AÃ±adir visualizaciÃ³n de fuentes
   - Soporte para mÃºltiples formatos de documentos

4. **ğŸ“ˆ Escalabilidad**
   - Soporte para mÃºltiples documentos
   - BÃºsqueda distribuida
   - IndexaciÃ³n incremental

## ğŸ“ Soporte

### Â¿Necesitas ayuda?

1. **ğŸ“‹ Verifica los logs** en la consola para mensajes de error detallados
2. **ğŸ“Š Revisa el reporte** en `evaluate/evaluation_report.txt`
3. **ğŸ” Comprueba** la configuraciÃ³n en `.env`
4. **ğŸ“š Consulta la documentaciÃ³n** de las dependencias

### Â¿Sigues teniendo problemas?

- ğŸ“§ EnvÃ­a un correo a [ron.h.castillo@gmail.com](mailto:ron.h.castillo@gmail.com)
- ğŸ”— Incluye los mensajes de error y los pasos para reproducir el problema

---

âœ¨ **Desarrollado con â¤ï¸ para el DesafÃ­o de Ingeniero de IA** âœ¨

---

### ğŸ“š Recursos Adicionales

- [DocumentaciÃ³n de RAGAS](https://github.com/explodinggradients/ragas)
- [GuÃ­a de FAISS](https://github.com/facebookresearch/faiss)
- [DocumentaciÃ³n de LM Studio](https://lmstudio.ai/docs/)

ğŸ¯ **Objetivo del Proyecto**: Crear un sistema de preguntas y respuestas confiable y escalable para documentaciÃ³n tÃ©cnica.