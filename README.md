# üß†‚ú® Sistema RAG Avanzado: Tu Experto en Aprendizaje Estad√≠stico

<div align="center">
  <img src="https://img.shields.io/badge/Python-3.10.18-306998?style=for-the-badge&logo=python&logoColor=white" alt="Python 3.10.18">
  <img src="https://img.shields.io/badge/LangChain-0.1.20-FF6B6B?style=for-the-badge&logo=python&logoColor=white" alt="LangChain">
  <img src="https://img.shields.io/badge/FAISS-Vector%20Store-00C4CC?style=for-the-badge&logo=facebook&logoColor=white" alt="FAISS">
  <img src="https://img.shields.io/badge/LLM-Llama--3.1-FF9900?style=for-the-badge&logo=llama&logoColor=white" alt="LLM Model">
  <img src="https://img.shields.io/badge/GPU-Accelerated-76B900?style=for-the-badge&logo=nvidia&logoColor=white" alt="GPU Accelerated">
</div>

<div align="center">
  <h3>üöÄ Transforma documentos en conocimiento accionable con IA</h3>
  <p>Una soluci√≥n todo-en-uno para extraer, procesar y generar conocimiento a partir de documentos t√©cnicos</p>
</div>

üë®‚Äçüíª **Autor**: Ronald Castillo Capino  
üìß **Contacto**: [ron.h.castillo@gmail.com](mailto:ron.h.castillo@gmail.com)

> üí° Este proyecto implementa un sistema avanzado de Preguntas y Respuestas (Q&A) que combina recuperaci√≥n de informaci√≥n con generaci√≥n de lenguaje natural, garantizando respuestas precisas, verificables y basadas en documentos espec√≠ficos.

## üêç Compatibilidad con Python 3.10.18

Este proyecto est√° desarrollado y probado espec√≠ficamente con Python 3.10.18. La elecci√≥n de esta versi√≥n se debe a:

- üöÄ **Rendimiento optimizado** para operaciones de procesamiento de lenguaje natural
- üîí **Estabilidad** en el ecosistema de IA/ML
- üì¶ **Compatibilidad** con bibliotecas clave como PyTorch y Transformers

### Verifica tu versi√≥n de Python

```bash
# Verificar versi√≥n instalada
python --version
# Deber√≠as ver: Python 3.10.18

# O alternativamente
python -c "import sys; print(f'Python {sys.version}')"
```

### Configuraci√≥n recomendada para entornos virtuales

```bash
# Crear entorno virtual con Python 3.10.18
py -3.10 -m venv venv  # Windows
# o
python3.10 -m venv venv  # Linux/Mac

# Activar el entorno
.\venv\Scripts\activate  # Windows
source venv/bin/activate  # Linux/Mac
```

## üåü ¬øQu√© hace este proyecto?

Imagina tener un asistente experto que puede leer y entender documentos t√©cnicos complejos, y responder a tus preguntas con precisi√≥n. ¬°Eso es exactamente lo que ofrece este sistema!

### üéØ Caracter√≠sticas principales

| Caracter√≠stica | Descripci√≥n | Beneficio |
|----------------|-------------|-----------|
| üîç **B√∫squeda Sem√°ntica** | Entiende el significado detr√°s de las palabras | Encuentra informaci√≥n relevante incluso con t√©rminos diferentes |
| üß† **Generaci√≥n Contextual** | Responde usando el modelo Llama-3.1 | Respuestas naturales y precisas con citas a las fuentes |
| üìä **Autoevaluaci√≥n** | Mide la calidad con m√©tricas RAGAS | Confianza en la precisi√≥n de las respuestas |
| ‚ö° **Rendimiento √ìptimo** | Optimizado para GPU NVIDIA | Respuestas r√°pidas incluso con documentos extensos |

### üéì Casos de Uso

- üìö **Estudiantes**: Entiende conceptos complejos de aprendizaje estad√≠stico
- üë®‚Äçüíª **Desarrolladores**: Integra conocimiento t√©cnico en tus aplicaciones
- üî¨ **Investigadores**: Analiza y extrae informaci√≥n de papers acad√©micos
- üè¢ **Empresas**: Crea una base de conocimiento interna accesible

### üèóÔ∏è C√≥mo funciona

```mermaid
flowchart LR
    A[üìÑ Documento PDF] --> B[üîç Procesamiento]
    B --> C[üìö Base de Conocimiento]
    D[‚ùì Pregunta] --> E[üß† Modelo RAG]
    C --> E
    E --> F[üí° Respuesta con Fuentes]
```

üí° **Caso de Uso Principal**: Sistema experto de preguntas y respuestas sobre el libro "An Introduction to Statistical Learning with Applications in Python", permitiendo a los usuarios obtener explicaciones claras y precisas sobre conceptos de aprendizaje estad√≠stico.

### üöÄ Desempe√±o con Aceleraci√≥n por GPU

El sistema est√° optimizado para aprovechar al m√°ximo la capacidad de procesamiento en paralelo de tu GPU local, ofreciendo tiempos de respuesta r√°pidos con el modelo Llama-3.1-Nemotron-Nano-8B. La configuraci√≥n incluye:

- **Contexto extendido**: 16,384 tokens
- **Capas GPU**: 30 capas (ajustable seg√∫n VRAM)
- **Procesamiento por lotes**: 512 tokens
- **Hilos de CPU**: 8 hilos

### üîß Requisitos de Hardware

| Componente | M√≠nimo | Recomendado |
|------------|--------|-------------|
| **GPU** | NVIDIA RTX 3060 | NVIDIA RTX 4090 o superior |
| **VRAM** | 12GB | 24GB+ |
| **RAM del Sistema** | 32GB | 64GB+ |
| **Almacenamiento** | 15GB libres | SSD NVMe |

### ‚öôÔ∏è Configuraci√≥n √ìptima

1. **Controladores NVIDIA**
   ```bash
   # Verificar instalaci√≥n de controladores
   nvidia-smi
   # Versi√≥n m√≠nima recomendada: 525.60.13
   ```

2. **Bibliotecas CUDA**
   - CUDA Toolkit 11.7 o superior
   - cuDNN 8.5 o superior
   - Verificar instalaci√≥n:
     ```bash
     nvcc --version
     ```

### üöÄ Optimizaciones Implementadas

- **Inferencia Acelerada por GPU**
  - Todas las operaciones del modelo se ejecutan en la GPU
  - Soporte para CUDA y cuBLAS para operaciones matriciales

- **Gesti√≥n Eficiente de Memoria**
  - Carga selectiva de capas del modelo
  - Optimizaci√≥n de memoria intermedia
  - Soporte para precisi√≥n mixta (FP16/FP32)

- **Procesamiento por Lotes**
  - Procesamiento paralelo de m√∫ltiples consultas
  - Ajuste autom√°tico del tama√±o de lote seg√∫n la VRAM disponible

### üìä Rendimiento Esperado

| Configuraci√≥n | Tokens/seg | Memoria GPU |
|---------------|------------|-------------|
| RTX 3060 (12GB) | 18-25 | ~12GB |
| RTX 3090 (24GB) | 30-40 | ~22GB |
| RTX 4090 (24GB) | 45-60 | ~24GB |

*Nota: El rendimiento puede variar seg√∫n la carga del sistema y la configuraci√≥n espec√≠fica.*

### Caracter√≠sticas Clave

- ‚úÖ **Respuestas Basadas en Contexto**: Cada respuesta est√° respaldada por fragmentos espec√≠ficos del documento
- üîç **B√∫squeda Sem√°ntica**: Encuentra informaci√≥n relevante incluso con consultas en lenguaje natural
- üìà **Evaluaci√≥n Continua**: Sistema integrado para medir y mejorar la calidad de las respuestas
- üöÄ **Rendimiento Optimizado**: Dise√±ado para funcionar eficientemente en hardware est√°ndar

## üéØ Objetivo

Desarrollar un asistente de IA que:

‚úÖ Proporcione respuestas precisas basadas en documentos espec√≠ficos  
‚úÖ Mantenga la trazabilidad de las fuentes de informaci√≥n  
‚úÖ Eval√∫e autom√°ticamente la calidad de las respuestas  
‚úÖ Sea f√°cil de implementar y mantener

## üìÇ Estructura del C√≥digo

El proyecto est√° organizado en los siguientes archivos principales:

### 1. `main.py`
M√≥dulo principal que implementa el sistema RAG con las siguientes caracter√≠sticas:

- **Clase Principal**: `RAGSystem`
  - Procesa documentos PDF y crea un √≠ndice de b√∫squeda sem√°ntica
  - Implementa b√∫squeda vectorial usando FAISS
  - Genera respuestas utilizando un modelo de lenguaje local

- **Configuraci√≥n**:
  ```python
  # Modelo por defecto
  MODEL_NAME = 'mistral-7b-instruct-v0.2'
  
  # Configuraci√≥n de fragmentaci√≥n
  CHUNK_SIZE = 4000  # Tama√±o de fragmentos de texto
  CHUNK_OVERLAP = 200  # Solapamiento entre fragmentos
  TOP_K_RETRIEVAL = 5  # N√∫mero de fragmentos a recuperar
  
  # Configuraci√≥n del modelo de lenguaje
  LLM_TEMPERATURE = 0.1
  MAX_TOKENS = 512
  ```

### 2. `evaluate.py`
M√≥dulo para evaluar el rendimiento del sistema RAG con las siguientes caracter√≠sticas:

- **Funcionalidades**:
  - Carga preguntas y respuestas de referencia desde `faq.json`
  - Implementa evaluaci√≥n con RAGAS (Retrieval-Augmented Generation Assessment)
  - Soporta modelos locales para embeddings y generaci√≥n
  - Genera reportes detallados de evaluaci√≥n

- **M√©tricas implementadas**:
  - `Faithfulness`: Mide qu√© tan fiel es la respuesta al contexto proporcionado
  - `Answer Relevancy`: Eval√∫a la relevancia de la respuesta respecto a la pregunta
  - `Context Precision`: Mide la precisi√≥n del contexto recuperado
  - `Context Recall`: Eval√∫a qu√© tan bien se recupera la informaci√≥n relevante

- **Configuraci√≥n**:
  ```python
  # Configuraci√≥n de evaluaci√≥n
  EVAL_SAMPLES = 2  # N√∫mero de ejemplos a evaluar
  LLM_TEMPERATURE = 0.1  # Controla la aleatoriedad de las respuestas
  MAX_TOKENS = 512  # M√°ximo n√∫mero de tokens por respuesta
  
  # Configuraci√≥n de modelos locales
  LOCAL_LLM_URL = "http://localhost:1234/v1"  # Endpoint del modelo local
  EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
  ```

### 3. `utils.py`
M√≥dulo de utilidades para el procesamiento de texto y gesti√≥n de vectores:

- **Funciones principales**:
  - `load_and_process_pdf()`: Extrae y procesa texto de archivos PDF
  - `chunk_text_semantically()`: Divide el texto en fragmentos significativos con solapamiento
  - `create_vector_store()`: Crea y gestiona el almac√©n vectorial FAISS

- **Caracter√≠sticas**:
  - Soporte para m√∫ltiples formatos de documentos
  - Tokenizaci√≥n inteligente que preserva la estructura sem√°ntica
  - Integraci√≥n con modelos de embeddings de Hugging Face
  - Gesti√≥n eficiente de memoria para documentos grandes

## üèóÔ∏è Arquitectura del Sistema

```mermaid
graph TD
    A[Usuario] -->|Pregunta| B[Procesamiento de Texto]
    B --> C[Generaci√≥n de Embeddings]
    C --> D[Base de Vectores FAISS]
    D -->|Contexto Relevante| E[Modelo de Lenguaje]
    E -->|Respuesta| A
    F[Documentos] -->|Procesamiento| G[Base de Conocimiento]
    G --> D
    H[M√≥dulo de Evaluaci√≥n] <-->|M√©tricas| E
```

### üîß Componentes Principales

#### 1. **Procesamiento de Documentos**
- **Extracci√≥n de Texto**: Utiliza PyPDF2 para extraer texto de documentos PDF
- **Limpieza de Texto**: Eliminaci√≥n de caracteres especiales, normalizaci√≥n de espacios
- **Tokenizaci√≥n**: Divisi√≥n del texto en unidades significativas
- **Fragmentaci√≥n Sem√°ntica**:
  - Tama√±o de fragmento: 4000 caracteres
  - Solapamiento: 200 caracteres
  - Preservaci√≥n de contexto entre fragmentos

#### 2. **Modelo de Embeddings**
- **Modelo**: `sentence-transformers/all-MiniLM-L6-v2`
- **Dimensi√≥n de Embeddings**: 384
- **Normalizaci√≥n**: Vectores unitarios para similitud coseno
- **Rendimiento**: Optimizado para equilibrio entre precisi√≥n y velocidad

#### 3. **Almacenamiento Vectorial (FAISS)**
- **√çndice**: `IndexFlatL2` para b√∫squeda exacta
- **M√©tricas**: Distancia euclidiana (L2)
- **Optimizaciones**:
  - B√∫squeda por lotes
  - Filtrado por umbral de similitud
  - Recuperaci√≥n de los 5 mejores resultados

#### 4. **Modelo de Lenguaje Local**
- **Modelo Base**: `Llama-3.1-Nemotron-Nano-8B-v1`
- **Ejecuci√≥n**:
  - Ejecutado localmente a trav√©s de LM Studio
  - Aceleraci√≥n por GPU para m√°ximo rendimiento
  - Versi√≥n cuantizada (Q4_K_S) para eficiencia
- **Configuraci√≥n**:
  - Temperatura: 0.1 (para respuestas deterministas)
  - Tokens m√°ximos: 2048
  - Contexto: 16,384 tokens
  - Capas GPU: 30 (ajustable seg√∫n VRAM)
  - Procesamiento por lotes: 512 tokens
  - Hilos CPU: 8
- **Prompt Engineering**:
  - Instrucciones claras para el modelo
  - Formato estructurado de respuestas
  - Manejo de incertidumbre

#### 5. **M√≥dulo de Evaluaci√≥n**
- **M√©tricas RAGAS**:
  - Faithfulness (Fidelidad)
  - Answer Relevancy (Relevancia de la respuesta)
  - Context Precision (Precisi√≥n del contexto)
  - Context Recall (Recuperaci√≥n del contexto)
  - Answer Similarity (Similitud de respuestas)
  - Answer Correctness (Correcci√≥n de respuestas)
  - Harmfulness (Contenido da√±ino)
- **Reportes**:
  - Salida detallada en consola
  - Archivo de registro estructurado
  - Estad√≠sticas agregadas

#### 6. **API y Servicios**
- **Interfaz de L√≠nea de Comandos (CLI)**
- **API RESTful** (opcional)
- **Integraci√≥n con LM Studio**
  - Endpoint: `http://localhost:1234/v1`
  - Soporte para streaming
  - Manejo de tiempo de espera

### üîÑ Flujo de Datos

1. **Ingreso de Consulta**
   - El usuario ingresa una pregunta en lenguaje natural
   - La consulta se normaliza y procesa

2. **B√∫squeda Sem√°ntica**
   - La consulta se convierte en embedding usando `sentence-transformers/all-MiniLM-L6-v2`
   - Se buscan los fragmentos m√°s similares en el √≠ndice FAISS
   - Se recuperan los 5 fragmentos m√°s relevantes

3. **Generaci√≥n de Respuesta**
   - Los fragmentos recuperados se combinan con un prompt estructurado
   - El modelo de lenguaje local (por defecto Mistral 7B) genera una respuesta contextualizada
   - Se incluyen referencias a las p√°ginas del documento original

4. **Evaluaci√≥n de Calidad**
   - El sistema calcula m√©tricas de evaluaci√≥n autom√°tica
   - Se genera un reporte detallado del rendimiento
   - Las interacciones se registran para an√°lisis posterior

## üöÄ Comenzando en 3, 2, 1...

### üì• Requisitos Previos

- Python 3.10.18 ([Descargar](https://www.python.org/downloads/release/python-31018/))
- Git
- CUDA Toolkit (para aceleraci√≥n GPU)
- pip (gestor de paquetes de Python)

### ‚öôÔ∏è Configuraci√≥n del Entorno (.env)

El archivo `.env` es fundamental para el funcionamiento del sistema. A continuaci√≥n se detallan todas las configuraciones disponibles:

### Configuraci√≥n B√°sica
```ini
# ===== Configuraci√≥n del Servidor Local =====
LOCAL_LLM_URL=http://192.168.100.5:1234/v1  # URL del servidor LM Studio
MODEL_NAME=mistral-7b-instruct-v0.2          # Nombre del modelo a utilizar
```

### Rutas de Archivos
```ini
# Ruta al modelo GGUF (descargado autom√°ticamente si no existe)
LOCAL_MODEL_PATH="C:\\Users\\ronal\\.cache\\lm-studio\\models\\jonahhenry\\mistral-7b-instruct-v0.2.Q4_K_M-GGUF\\mistral-7b-instruct-v0.2.Q4_K_M.gguf"
PDF_PATH=data/PDF-GenAI-Challenge.pdf  # Ruta al documento PDF de entrada
CACHE_DIR=./cache                     # Directorio para cach√©
```

### Configuraci√≥n del Modelo
```ini
# ===== Configuraci√≥n de Generaci√≥n =====
LLM_TEMPERATURE=0.1      # Controla la creatividad (0-1, m√°s bajo = m√°s determinista)
TOP_P=0.9                # Muestreo de n√∫cleo (nucleus sampling)
MAX_TOKENS=128           # Longitud m√°xima de las respuestas
FREQUENCY_PENALTY=0.0    # Penalizaci√≥n por repetici√≥n de tokens
PRESENCE_PENALTY=0.6     # Penalizaci√≥n por repetici√≥n de temas
```

### Rendimiento y Recursos
```ini
# ===== Configuraci√≥n de Rendimiento =====
N_CTX=2048               # Tama√±o del contexto
N_BATCH=1                # Tama√±o del lote para inferencia
N_GPU_LAYERS=0           # Capas a ejecutar en GPU (0 = CPU, 99 = todas en GPU)
N_THREADS=4              # Hilos de CPU a utilizar
```

### Procesamiento de Documentos
```ini
# ===== Configuraci√≥n de Fragmentaci√≥n =====
CHUNK_SIZE=4000          # Tama√±o de los fragmentos de texto
CHUNK_OVERLAP=200        # Solapamiento entre fragmentos
TOP_K_RETRIEVAL=5        # N√∫mero de fragmentos a recuperar
SIMILARITY_THRESHOLD=0.7 # Umbral de similitud m√≠nimo
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
```

### Evaluaci√≥n
```ini
# ===== Configuraci√≥n de Evaluaci√≥n =====
EVAL_SAMPLES=0           # N√∫mero de ejemplos a evaluar (0 = todos)
EVALUATION_INPUT_PATH=evaluate/faq.json
EVALUATION_OUTPUT_PATH=evaluate/evaluation_report.txt
```

### Registro (Logging)
```ini
# ===== Configuraci√≥n de Logs =====
LOG_LEVEL=INFO           # Nivel de detalle del registro
LOG_FILE=rag_system.log  # Archivo de registro

# Configuraci√≥n de entorno
GIT_PYTHON_REFRESH=quiet
TRANSFORMERS_OFFLINE=1   # Modo offline para transformers
RAGAS_DO_NOT_TRACK=true  # Deshabilitar telemetr√≠a de RAGAS
```

## ‚öôÔ∏è Instalaci√≥n Paso a Paso

```bash
# 1. Clona el repositorio
üêö git clone https://github.com/tu-usuario/tu-proyecto.git
üìÇ cd tu-proyecto

# 2. Crea y activa el entorno virtual
üêç python -m venv venv
# En Windows:
üîå .\venv\Scripts\activate
# En Linux/Mac:
# üîå source venv/bin/activate

# 3. Instala las dependencias
üì¶ pip install --upgrade pip
üì¶ pip install -r requirements.txt

# 4. Configura las variables de entorno
üîß copy .env.example .env  # Windows
# En Linux/Mac:
# üîß cp .env.example .env

# 5. Edita el archivo .env con tus configuraciones
‚öôÔ∏è notepad .env  # O usa tu editor favorito
```

### üéÆ Uso B√°sico

```python
# üìÇ main.py
from __future__ import annotations  # Para mejor compatibilidad de tipos
import os
import sys
from pathlib import Path
from typing import Dict, Any

# Verificar versi√≥n de Python
if sys.version_info < (3, 10):
    raise RuntimeError("Se requiere Python 3.10.18 o superior")

# Configuraci√≥n de rutas
PROJECT_ROOT = Path(__file__).parent
sys.path.append(str(PROJECT_ROOT))

# Cargar variables de entorno
from dotenv import load_dotenv
load_dotenv()

# Importaciones locales
from main import RAGSystem

# Cargar variables de entorno
load_dotenv()

# üèóÔ∏è Inicializa el sistema con tu documento
print("üöÄ Inicializando el sistema RAG...")
rag = RAGSystem("documentos/libro_estadistica.pdf")
rag.setup_system()  # ‚è≥ Esto puede tomar unos minutos la primera vez

# ‚ùì Ejemplo de pregunta
pregunta = "¬øC√≥mo funciona la regresi√≥n log√≠stica?"
print(f"\nüîç Procesando pregunta: {pregunta}")

# üéØ Generar respuesta
respuesta = rag.generate_response(
    pregunta,
    temperature=0.1,  # Controla la creatividad (0-1)
    max_tokens=512    # Longitud m√°xima de la respuesta
)

# ‚ú® Mostrar resultados
print("\n" + "="*80)
print(f"üîç Pregunta: {pregunta}")
print("-"*80)
print(f"üí° Respuesta: {respuesta['answer']}")
print("-"*80)
print(f"üìö Fuentes: {', '.join(respuesta['sources'])}")
print(f"üéØ Confianza: {respuesta['confidence']:.1%}")
print("="*80 + "\n")
```

### üéØ Ejemplos Pr√°cticos con Tipado Est√°tico

#### 1. B√∫squeda de Conceptos con Tipado
```python
def obtener_explicacion(rag: RAGSystem, concepto: str, temp: float = 0.7) -> Dict[str, Any]:
    """Obtiene una explicaci√≥n detallada de un concepto estad√≠stico.
    
    Args:
        rag: Instancia de RAGSystem
        concepto: T√©rmino o concepto a explicar
        temp: Temperatura para la generaci√≥n (0-1)
        
    Returns:
        Dict con la respuesta y metadatos
    """
    respuesta = rag.generate_response(
        f"Explica el concepto de {concepto} con un ejemplo pr√°ctico",
        temperature=min(max(temp, 0), 1),  # Asegurar valor entre 0 y 1
        max_tokens=512
    )
    return respuesta

# Uso
explicacion = obtener_explicacion(rag, "teorema de Bayes", temp=0.7)
print(f"üìù {explicacion['answer']}")
print(f"üìä Confianza: {explicacion['confidence']:.1%}")
```

#### 2. Comparaci√≥n de Algoritmos
```python
respuesta = rag.generate_response(
    "Compara los √°rboles de decisi√≥n con los bosques aleatorios",
    max_tokens=1000  # L√≠mite de longitud
)
```

#### 3. Generaci√≥n de Ejemplos de C√≥digo
```python
respuesta = rag.generate_response(
    "Muestra un ejemplo de implementaci√≥n de regresi√≥n lineal en Python",
    include_code=True
)
```

### 2. Evaluaci√≥n del Rendimiento
```python
from evaluate import run_ragas_evaluation, create_evaluation_dataset
from datasets import Dataset

# Cargar datos de evaluaci√≥n
eval_data = create_evaluation_dataset()

# Ejecutar evaluaci√≥n
results = run_ragas_evaluation(eval_data)

# Mostrar resultados
print("\n=== Resultados de la Evaluaci√≥n ===")
for metric, score in results.items():
    print(f"{metric}: {score:.4f}")
```

### 3. Configuraci√≥n Avanzada
```python
# Personalizar la evaluaci√≥n
os.environ["RAGAS_EMBEDDINGS"] = "huggingface"
os.environ["RAGAS_DO_NOT_TRACK"] = "true"

# Usar un modelo local diferente
os.environ["LOCAL_LLM_URL"] = "http://localhost:1234/v1"

# Configurar el nivel de detalle del logging
import logging
logging.basicConfig(level=logging.INFO)
```

## ‚öôÔ∏è Configuraci√≥n Avanzada

### üîß Variables de Entorno Clave

Crea un archivo `.env` en la ra√≠z del proyecto con estas configuraciones:

```ini
# üéØ Configuraci√≥n del Modelo
MODEL_NAME=llama-3.1-nemotron-nano-8b-v1
LOCAL_MODEL_PATH=modelos/llama-3.1.gguf  # Ruta a tu modelo

# ‚ö° Rendimiento
GPU_LAYERS=30  # Capas a cargar en GPU (ajustar seg√∫n VRAM)
BATCH_SIZE=512  # Tama√±o de lote para inferencia
THREADS=8      # Hilos de CPU para procesamiento

# üîç B√∫squeda
TOP_K_RESULTS=5       # N√∫mero de fragmentos a recuperar
SIMILARITY_THRESHOLD=0.7  # Umbral de similitud m√≠nimo

# üìä Evaluaci√≥n
EVALUATION_SAMPLES=10  # N√∫mero de ejemplos para evaluaci√≥n
EVAL_TEMPERATURE=0.1   # Temperatura para generaci√≥n en evaluaci√≥n
```

### üõ†Ô∏è Personalizaci√≥n Avanzada

#### 1. Ajuste de Par√°metros del Modelo
```python
# üìÇ main.py
rag = RAGSystem(
    "documento.pdf",
    model_params={
        'temperature': 0.3,  # Controla la creatividad (0-1)
        'max_tokens': 1024,  # Longitud m√°xima de respuesta
        'top_p': 0.9,       # Muestreo de n√∫cleo
        'repeat_penalty': 1.1  # Penalizaci√≥n por repetici√≥n
    }
)
```

#### 2. Personalizaci√≥n del Prompt
Puedes modificar el prompt del sistema para adaptarlo a tus necesidades:

```python
# üìÇ prompts/custom_prompt.txt
Eres un experto en aprendizaje estad√≠stico. Responde de manera clara y concisa.

Contexto:
{context}

Pregunta: {question}

Respuesta (incluye las p√°ginas de referencia [p.XX]):
```

Luego c√°rgalo as√≠:
```python
rag = RAGSystem("documento.pdf", prompt_file="prompts/custom_prompt.txt")
```

# Configuraci√≥n de Rendimiento
N_CTX=16384
N_GPU_LAYERS=30
N_BATCH=512
N_THREADS=8

# Configuraci√≥n de Generaci√≥n
LLM_TEMPERATURE=0.1
MAX_TOKENS=2048

# Configuraci√≥n de Fragmentaci√≥n
CHUNK_SIZE=4000
CHUNK_OVERLAP=200
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Configuraci√≥n de Recuperaci√≥n
TOP_K_RETRIEVAL=5
CONTEXT_WINDOW=131072
```

## üìÅ Estructura del Proyecto

```
.
‚îú‚îÄ‚îÄ data/                           # Directorio de datos
‚îÇ   ‚îî‚îÄ‚îÄ PDF-GenAI-Challenge.pdf    # Documento fuente para el sistema RAG
‚îÇ
‚îú‚îÄ‚îÄ evaluate/                      # Resultados de evaluaci√≥n
‚îÇ   ‚îî‚îÄ‚îÄ evaluation_report.txt      # Reporte detallado de m√©tricas
‚îÇ
‚îú‚îÄ‚îÄ .env                   # Plantilla de configuraci√≥n
‚îú‚îÄ‚îÄ main.py                       # Punto de entrada principal
‚îú‚îÄ‚îÄ evaluate.py                   # M√≥dulo de evaluaci√≥n
‚îú‚îÄ‚îÄ utils.py                      # Utilidades y helpers
‚îú‚îÄ‚îÄ requirements.txt              # Dependencias del proyecto
‚îî‚îÄ‚îÄ README.md                     # Documentaci√≥n
```

### üìã Archivos Principales

#### `main.py`
Punto de entrada principal que implementa:
- Carga y procesamiento de documentos
- Interfaz de l√≠nea de comandos
- Integraci√≥n de componentes RAG
- Manejo de errores y logging

#### `evaluate.py`
M√≥dulo de evaluaci√≥n que incluye:
- Implementaci√≥n de m√©tricas RAGAS
- Generaci√≥n de reportes detallados
- Herramientas para an√°lisis comparativo
- Visualizaci√≥n de resultados

#### `utils.py`
Funciones auxiliares para:
- Procesamiento de texto y PDF
- Manejo de embeddings
- Utilidades de sistema
- Configuraci√≥n y logging

#### `requirements.txt`
Lista de dependencias con versiones espec√≠ficas para garantizar compatibilidad.

#### `.env`
Archivo de configuraci√≥n que debe contener:
- Rutas a modelos
- Configuraciones de ejecuci√≥n
- Par√°metros del sistema

## ‚öôÔ∏è Instalaci√≥n y Configuraci√≥n

### 1. üõ†Ô∏è Requisitos Previos

- Python 3.8 o superior
- pip (gestor de paquetes de Python)
- Git (opcional, para clonar el repositorio)
- LM Studio (para ejecutar modelos locales)
- Al menos 8GB de RAM (16GB recomendado)

### 2. üèóÔ∏è Configuraci√≥n del Entorno

```bash
# 1. Clonar el repositorio (opcional)
git clone <repo-url>
cd rag_challenge

# 2. Crear y activar entorno virtual
python -m venv rag_env
source rag_env/bin/activate  # Linux/Mac
# o
rag_env\Scripts\activate     # Windows

# 3. Actualizar pip
python -m pip install --upgrade pip

# 4. Instalar dependencias
pip install -r requirements.txt
```

### 3. üîê Configuraci√≥n de Variables de Entorno

1. Copiar el archivo de ejemplo:
   ```bash
   cp .env.example .env
   ```

2. Editar el archivo `.env` con tus configuraciones:
   ```env
   # ===== Configuraci√≥n del Modelo =====
   LOCAL_LLM_URL=http://localhost:1234/v1
   MODEL_NAME=llama-3.1-nemotron-nano-8b-v1
   
   # ===== Configuraci√≥n de Embeddings =====
   EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
   
   # ===== Configuraci√≥n de Procesamiento =====
   CHUNK_SIZE=4000
   CHUNK_OVERLAP=200
   TOP_K_RETRIEVAL=5
   
   # ===== Configuraci√≥n del Modelo =====
   LLM_TEMPERATURE=0.1
   MAX_TOKENS=1024
   CONTEXT_WINDOW=2048
   
   # ===== Configuraci√≥n de Evaluaci√≥n =====
   EVAL_SAMPLES=5
   EVALUATION_OUTPUT_PATH=evaluate/evaluation_report.txt
   
   # ===== Configuraci√≥n del Sistema =====
   LOG_LEVEL=INFO
   CACHE_DIR=./cache
   ```

### 4. üì¶ Dependencias Principales

El archivo `requirements.txt` contiene todas las dependencias necesarias:

```
# Procesamiento de documentos
PyPDF2>=3.0.0
python-dotenv>=1.0.0

# Procesamiento de lenguaje natural
sentence-transformers>=2.2.2
numpy>=1.24.0

# Almacenamiento vectorial
faiss-cpu>=1.7.4  # o faiss-gpu si tienes CUDA

# Modelo de lenguaje
llama-cpp-python>=0.2.0

# Evaluaci√≥n
ragas>=0.0.21
datasets>=2.14.0

# Utilidades
tqdm>=4.65.0
colorama>=0.4.6
python-dotenv>=1.0.0
```

### 5. üöÄ Configuraci√≥n del Modelo Local

#### Descarga e Instalaci√≥n del Modelo

1. **Descargar el modelo**:
   - Nombre: `Llama-3.1-Nemotron-Nano-4B-v1.1-GGUF`
   - Tama√±o: ~2.5GB (versi√≥n cuantizada Q4_K_M)
   - Ubicaci√≥n por defecto: `C:\Users\[usuario]\.cache\lm-studio\models\`

2. **Configuraci√≥n en LM Studio**:
   - Abrir LM Studio y seleccionar "Download a model"
   - Buscar: `llama-3.1-nemotron-nano-8b-v1-GGUF`
   - Descargar la versi√≥n `Q4_K_M` para el mejor equilibrio entre rendimiento y calidad

3. **Habilitar Aceleraci√≥n por GPU**:
   - Ir a Configuraci√≥n ‚Üí Modelo
   - Seleccionar "Auto" o tu GPU espec√≠fica en "GPU Layers"
   - Activar "Use CUDA"
   - Establecer "Context Length" a 2048 tokens

4. **Iniciar el Servidor Local**:
   - Ir a la pesta√±a "Local Server"
   - Asegurarse de que "GPU Offload" est√© activado
   - Hacer clic en "Start Server"
   - Verificar que la URL sea `http://localhost:1234/v1`

5. **Verificar la Configuraci√≥n de GPU**:
   - Abrir el Administrador de Tareas de Windows
   - Ir a la pesta√±a "Rendimiento"
   - Verificar que la GPU muestre actividad durante la inferencia
   - Confirmar que la memoria de GPU se est√© utilizando

#### Configuraci√≥n Recomendada para √ìptimo Rendimiento

```env
# En tu archivo .env
LOCAL_LLM_URL=http://localhost:1234/v1
MODEL_NAME=llama-3.1-nemotron-nano-8b-v1
LLM_TEMPERATURE=0.1
MAX_TOKENS=1024
CONTEXT_WINDOW=2048
```

2. **Preparar los datos**:
   ```bash
   mkdir -p data
   cp /ruta/a/tu/documento.pdf data/PDF-GenAI-Challenge.pdf
   ```

3. **Verificar la instalaci√≥n**:
   ```bash
   python -c "import torch; print(f'PyTorch: {torch.__version__}')"
   python -c "import transformers; print(f'Transformers: {transformers.__version__}')"
   ```

### 6. üß™ Prueba R√°pida

```bash
# Ejecutar una consulta de prueba
echo "¬øQu√© es el aprendizaje autom√°tico?" | python main.py

# Ejecutar evaluaci√≥n b√°sica
python evaluate.py --samples 3
```

### üìù Notas de Instalaci√≥n

- Para mejor rendimiento, se recomienda usar una GPU compatible con CUDA
- El primer inicio puede tardar varios minutos mientras se descargan los modelos
- Verifica que el puerto 1234 est√© disponible para LM Studio

## üíª Uso del Sistema

### 1. Modos de Operaci√≥n

#### Modo Interactivo
```bash
# Iniciar el sistema en modo interactivo
python main.py

# Ejemplo de sesi√≥n:
Bienvenido al Sistema RAG de Control de Calidad
> ¬øSobre qu√© tema te gustar√≠a consultar?
```

#### Modo por L√≠nea de Comandos
```bash
# Hacer una pregunta espec√≠fica
python main.py --pregunta "¬øQu√© es el trade-off entre sesgo y varianza?"

# Procesar un archivo con m√∫ltiples preguntas
python main.py --archivo preguntas.txt --salida respuestas.json

# Opciones adicionales
python main.py \
  --pregunta "Explique la regresi√≥n lineal" \
  --temperatura 0.3 \
  --max-tokens 500 \
  --mostrar-fuentes
```

### 2. Ejemplos de Preguntas

#### Conceptos B√°sicos
```
- "¬øQu√© es el aprendizaje supervisado?"
- "Explique el concepto de validaci√≥n cruzada"
- "¬øCu√°les son las ventajas de los √°rboles de decisi√≥n?"
```

#### Comparaciones
```
- "Compare ridge y lasso regression"
- "Diferencia entre bagging y boosting"
- "Ventajas de SVM sobre regresi√≥n log√≠stica"
```

#### Aplicaciones Pr√°cticas
```
- "¬øC√≥mo manejar datos faltantes en un dataset?"
- "T√©cnicas para tratar el desbalance de clases"
- "M√©todos de selecci√≥n de caracter√≠sticas"
```

### 3. Opciones Avanzadas

#### Configuraci√≥n de B√∫squeda
```bash
# Ajustar el n√∫mero de fragmentos recuperados
python main.py --top-k 3

# Cambiar el umbral de similitud (0-1)
python main.py --umbral-similitud 0.75
```

#### Control de Salida
```bash
# Mostrar solo la respuesta sin fuentes
python main.py --pregunta "..." --formato simple

# Generar salida en formato JSON
python main.py --pregunta "..." --formato json

# Guardar resultados en un archivo
python main.py --pregunta "..." --salida resultado.txt
```

## üìä Evaluaci√≥n del Sistema

### 1. M√©tricas Implementadas

El sistema utiliza el framework RAGAS para evaluar la calidad de las respuestas con las siguientes m√©tricas:

| M√©trica | Rango √ìptimo | Descripci√≥n |
|---------|--------------|-------------|
| **Faithfulness** | 0.8 - 1.0 | Mide si la respuesta se basa √∫nicamente en el contexto |
| **Answer Relevancy** | > 0.7 | Eval√∫a la relevancia de la respuesta |
| **Context Precision** | > 0.6 | Precisi√≥n de los fragmentos recuperados |
| **Context Recall** | > 0.7 | Capacidad de recuperar informaci√≥n relevante |
| **Answer Similarity** | > 0.75 | Comparaci√≥n con respuestas de referencia |
| **Answer Correctness** | > 0.8 | Precisi√≥n f√°ctica de la respuesta |
| **Harmfulness** | < 0.2 | Detecci√≥n de contenido potencialmente da√±ino |

### 2. Ejecuci√≥n de la Evaluaci√≥n

#### Evaluaci√≥n B√°sica
```bash
# Evaluar con 5 ejemplos (valor por defecto)
python evaluate.py
```

#### Evaluaci√≥n Personalizada
```bash
# Especificar n√∫mero de muestras
python evaluate.py --muestras 10

# Evaluar m√©tricas espec√≠ficas
python evaluate.py --metricas fidelidad relevancia

# Generar reporte en formato JSON
python evaluate.py --formato json --salida resultados.json
```

#### Evaluaci√≥n con Conjunto de Datos Personalizado
```bash
# Usar un archivo JSON con preguntas y respuestas de referencia
python evaluate.py --dataset datos_evaluacion.json
```

### 3. Interpretaci√≥n de Resultados

#### Ejemplo de Salida
```
========================================
   RESULTADOS DE LA EVALUACI√ìN RAGAS   
========================================

- Faithfulness: 0.87
  ‚úì Excelente: La respuesta se basa completamente en el contexto

- Answer Relevancy: 0.82
  ‚úì Muy buena: La respuesta es altamente relevante a la pregunta

- Context Precision: 0.75
  ‚úì Buena: La mayor√≠a de los fragmentos recuperados son relevantes

- Context Recall: 0.68
  ‚úì Aceptable: Se recupera la mayor parte de la informaci√≥n relevante

- Answer Similarity: 0.79
  ‚úì Buena: La respuesta es similar a la referencia esperada

- Answer Correctness: 0.83
  ‚úì Muy buena: La informaci√≥n proporcionada es correcta

- Harmfulness: 0.05
  ‚úì Seguro: No se detect√≥ contenido da√±ino

----------------------------------------
Puntuaci√≥n Promedio: 0.79
Estado General: Buen rendimiento
----------------------------------------

üìù Recomendaciones:
- Mejorar la recuperaci√≥n de contexto para aumentar el recall
- Verificar posibles casos de informaci√≥n faltante en las respuestas
```

### 4. An√°lisis de Resultados

#### Archivos Generados
- `evaluate/evaluation_report.txt`: Reporte detallado en formato de texto
- `evaluate/metrics/`: Directorio con m√©tricas hist√≥ricas
- `evaluate/failed_cases.json`: Casos que requieren revisi√≥n manual

#### Visualizaci√≥n de M√©tricas
```bash
# Generar gr√°ficos de tendencias
python evaluate.py --graficar
```

### 5. Personalizaci√≥n de la Evaluaci√≥n

Puedes modificar el archivo `evaluate.py` para:
- Ajustar los umbrales de las m√©tricas
- Agregar nuevas m√©tricas personalizadas
- Cambiar el conjunto de datos de evaluaci√≥n
- Modificar los prompts de evaluaci√≥n

### 6. Integraci√≥n Continua

El sistema puede integrarse en pipelines CI/CD para monitorear el rendimiento a lo largo del tiempo:

```yaml
# Ejemplo de configuraci√≥n para GitHub Actions
name: Evaluaci√≥n RAG

on: [push, pull_request]

jobs:
  evaluate:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    - name: Run evaluation
      run: |
        python evaluate.py --samples 10
    - name: Upload results
      uses: actions/upload-artifact@v3
      with:
        name: evaluation-results
        path: evaluate/
```

## üö® Soluci√≥n de Problemas

### 1. Problemas de Inicio

**S√≠ntoma**: Error al iniciar la aplicaci√≥n
```
[ERROR] No se pudo cargar el modelo: ConnectionError
```
**Soluci√≥n**:
1. Verifica que LM Studio est√© ejecut√°ndose y que la URL en `.env` sea correcta
2. Confirma la URL en el archivo `.env`
3. Revisa los logs para mensajes adicionales

### 2. Problemas de Rendimiento

**S√≠ntoma**: Respuestas lentas o tiempo de espera agotado
```
[WARNING] La generaci√≥n est√° tardando m√°s de lo esperado
```
**Soluci√≥n**:
```bash
# Reducir la carga del sistema
export TOP_K_RETRIEVAL=3
export MAX_TOKENS=512

# Para sistemas con GPU limitada
export CUDA_VISIBLE_DEVICES=0  # Usar solo la primera GPU
```

### 3. Optimizaci√≥n de GPU para M√°ximo Rendimiento

**S√≠ntoma**: Bajo uso de GPU o rendimiento por debajo de lo esperado
```
[INFO] Uso de GPU por debajo del 50%
```
**Soluci√≥n**:
1. Verifica que CUDA est√© correctamente instalado:
   ```bash
   nvidia-smi  # Debe mostrar el uso de GPU
   python -c "import torch; print(f'CUDA disponible: {torch.cuda.is_available()}'
   ```
2. Ajusta el tama√±o de lote para mejor uso de GPU:
   ```python
   # En tu configuraci√≥n
   os.environ['BATCH_SIZE'] = '8'  # Aumentar para mejor uso de GPU
   ```
3. Habilita la precisi√≥n mixta:
   ```python
   from torch.cuda.amp import autocast
   
   with autocast():
       # Tu c√≥digo de inferencia aqu√≠
   ```

### 4. Problemas de Memoria

**S√≠ntoma**: Errores de memoria insuficiente
```
[ERROR] Error al asignar memoria: CUDA out of memory
```
**Soluci√≥n**:
1. Reduce el tama√±o del lote:
   ```python
   # En tu c√≥digo Python
   import os
   os.environ['BATCH_SIZE'] = '4'
   ```
2. Usa precisi√≥n mixta:
   ```python
   model.half()  # Usar precisi√≥n FP16
   ```
3. Libera memoria de la GPU:
   ```python
   import torch
   torch.cuda.empty_cache()
   ```

### 4. Problemas con los Embeddings

**S√≠ntoma**: Baja precisi√≥n en las b√∫squedas
```
[WARNING] No se encontraron fragmentos relevantes
```
**Soluci√≥n**:
- Verifica que el modelo de embeddings est√© correctamente cargado
- Considera ajustar el umbral de similitud
- Revisa la calidad del texto de entrada

## üöÄ Pr√≥ximas Mejoras

### Mejoras en Curso

1. **Optimizaci√≥n de Rendimiento**
   - [ ] Soporte para cuantizaci√≥n de modelos
   - [ ] Cach√© de embeddings en disco
   - [ ] Procesamiento por lotes mejorado

2. **Nuevas Funcionalidades**
   - [ ] Soporte para m√∫ltiples formatos de documentos
   - [ ] Integraci√≥n con m√°s bases de datos vectoriales
   - [ ] Sistema de plugins para extensiones

3. **Mejoras en la Interfaz**
   - [ ] Interfaz web interactiva
   - [ ] Panel de control de m√©tricas
   - [ ] Visualizaci√≥n de grafos de conocimiento

### Caracter√≠sticas Futuras

1. **Soporte Multimodal**
   - Procesamiento de im√°genes y tablas
   - B√∫squeda sem√°ntica en m√∫ltiples formatos
   - Respuestas enriquecidas con visualizaciones

2. **Aprendizaje Autom√°tico**
   - Mejora continua basada en feedback
   - Detecci√≥n autom√°tica de temas
   - Generaci√≥n de res√∫menes ejecutivos

3. **Colaboraci√≥n**
   - Compartir fragmentos de documentos
   - Anotaciones colaborativas
   - Sistema de revisi√≥n por pares

## ü§ù Contribuciones

¬°Las contribuciones son bienvenidas! Por favor, lee nuestra [gu√≠a de contribuci√≥n](CONTRIBUTING.md) para m√°s detalles.

## üìÑ Licencia

Este proyecto est√° bajo la Licencia MIT. Ver el archivo [LICENSE](LICENSE) para m√°s detalles.

## üìû Contacto

Para consultas o soporte, por favor contacta a [ron.h.castillo@gmail.com](mailto:ron.h.castillo@gmail.com)

---

<div align="center">
  Hecho con ‚ù§Ô∏è por Ronald Castillo Capino
</div>
- **Extracci√≥n de texto** con PyPDF2
- **Divisi√≥n inteligente** que mantiene el contexto
- **Metadatos** para seguimiento de fuentes

### üî¢ Generaci√≥n de Embeddings
- **Modelo**: `sentence-transformers/all-MiniLM-L6-v2`
- **Normalizaci√≥n**: Vectores unitarios para similitud coseno
- **Rendimiento**: Optimizado para b√∫squeda r√°pida

### üîç Recuperaci√≥n
- **Motor**: FAISS para b√∫squeda vectorial
- **Estrategia**: Top-5 fragmentos m√°s relevantes
- **Rendimiento**: Respuestas en tiempo real

### ü§ñ Generaci√≥n
- **Modelo Local**: `llama-3.1-nemotron-nano-8b-v1`
- **T√©cnicas Avanzadas**:
  - Prompt engineering
  - Control de contexto
  - Verificaci√≥n de hechos

## üìà Resultados Esperados

El sistema est√° dise√±ado para superar los siguientes umbrales:

| M√©trica | Objetivo | Explicaci√≥n |
|---------|----------|-------------|
| üéØ Fidelidad | > 0.90 | Las respuestas se basan estrictamente en el contexto |
| üéØ Relevancia | > 0.85 | Las respuestas responden directamente a la pregunta |
| üéØ Precisi√≥n | > 0.80 | Los fragmentos recuperados son relevantes |
| üéØ Recall | > 0.75 | Se recupera la mayor√≠a de la informaci√≥n relevante |

üí° Estos valores pueden variar seg√∫n la calidad del documento fuente y la complejidad de las preguntas.

## üö® Soluci√≥n de Problemas

### Problemas Comunes y Soluciones:

1. **üîë Error de Autenticaci√≥n**
   ```
   [ERROR] Error de autenticaci√≥n con el modelo
   ```
   üîß **Soluci√≥n**: Verifica que LM Studio est√© ejecut√°ndose y que la URL en `.env` sea correcta

2. **üìÑ Archivo PDF no encontrado**
   ```
   [ERROR] No se encontr√≥ el archivo PDF
   ```
   üîß **Soluci√≥n**: Aseg√∫rate de que el archivo existe en `data/PDF-GenAI-Challenge.pdf`

3. **üêå Rendimiento lento**
   ```
   [INFO] La generaci√≥n de respuestas est√° tardando m√°s de lo esperado
   ```
   üîß **Soluci√≥n**:
   - Reduce el n√∫mero de fragmentos recuperados
   - Usa un modelo m√°s peque√±o
   - Verifica el rendimiento de tu hardware

4. **üíæ Problemas de memoria**
   ```
   MemoryError: No se puede asignar memoria
   ```
   üîß **Soluci√≥n**:
   - Reduce el tama√±o del lote de procesamiento
   - Cierra otras aplicaciones que consuman mucha memoria
   - Considera usar un equipo con m√°s RAM

## üöÄ Pr√≥ximos Pasos

### Mejoras Planificadas:

1. **üéØ Mejora de Precisi√≥n**
   - Implementar re-ranking de resultados
   - A√±adir verificaci√≥n cruzada de hechos
   - Mejorar la recuperaci√≥n de contexto

2. **‚ö° Rendimiento**
   - Optimizar el uso de memoria
   - Implementar cach√© de embeddings
   - Soporte para procesamiento por lotes

3. **üåê Interfaz de Usuario**
   - Desarrollar interfaz web interactiva
   - A√±adir visualizaci√≥n de fuentes
   - Soporte para m√∫ltiples formatos de documentos

4. **üìà Escalabilidad**
   - Soporte para m√∫ltiples documentos
   - B√∫squeda distribuida
   - Indexaci√≥n incremental

## üìû Soporte

### ¬øNecesitas ayuda?

1. **üìã Verifica los logs** en la consola para mensajes de error detallados
2. **üìä Revisa el reporte** en `evaluate/evaluation_report.txt`
3. **üîç Comprueba** la configuraci√≥n en `.env`
4. **üìö Consulta la documentaci√≥n** de las dependencias

### ¬øSigues teniendo problemas?

- üìß Env√≠a un correo a [ron.h.castillo@gmail.com](mailto:ron.h.castillo@gmail.com)
- üîó Incluye los mensajes de error y los pasos para reproducir el problema

---

‚ú® **Desarrollado con ‚ù§Ô∏è para el Desaf√≠o de Ingeniero de IA** ‚ú®

---

### üìö Recursos Adicionales

- [Documentaci√≥n de RAGAS](https://github.com/explodinggradients/ragas)
- [Gu√≠a de FAISS](https://github.com/facebookresearch/faiss)
- [Documentaci√≥n de LM Studio](https://lmstudio.ai/docs/)

üéØ **Objetivo del Proyecto**: Crear un sistema de preguntas y respuestas confiable y escalable para documentaci√≥n t√©cnica.